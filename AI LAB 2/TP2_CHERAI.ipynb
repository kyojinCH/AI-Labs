{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 2 : Sentiment analysis, a Text Classification Task"
   ],
   "metadata": {
    "id": "32U-eSMtcK7x"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pEJM0h_uctzV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d8619d62-28e6-4ba4-a625-0ddd22a1b10b"
   },
   "source": [
    "!pip install gdown\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "!gdown --id 12Jv1GpSBRCItwxP8LXBXDhOAow609IUP # Dowload a SentimentCSV file\n",
    "\n",
    "# we fix the seeds to get consistent results before every training loop in what follows\n",
    "def fix_seed(seed=234):\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)\n",
    "fix_seed()"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (4.6.4)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from gdown) (4.12.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from beautifulsoup4->gdown) (2.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from requests[socks]->gdown) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from requests[socks]->gdown) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->gdown) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: gdown [-h] [-V] [-O OUTPUT] [-q] [--fuzzy] [--id] [--proxy PROXY]\n",
      "             [--speed SPEED] [--no-cookies] [--no-check-certificate]\n",
      "             [--continue] [--folder] [--remaining-ok]\n",
      "             url_or_id\n",
      "gdown: error: unrecognized arguments: # Dowload a SentimentCSV file\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE8B9-L8U0aZ"
   },
   "source": [
    "## About this lab\n",
    "\n",
    "In this notebook, we are going to build state-of-the art models for text classification using the example of sentiment analysis. To be more precise, we will build a feed-forward neural network (FFNN) and a convolutional neural network (CNN). We will look into the details of data preparation, functioning of each model and how the performance of those NNs could be measured efficiently. We will start our work using a toy corpus. Further you can extend your knowledge and use a larger dataset. Again we are using [pytorch](https://www.pytorch.org), an open source deep learning platform, as our backbone library in the course.\n",
    "\n",
    "Goal of this lab :\n",
    "* Preprocess some Text Dataset\n",
    "* Train models for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Toying Around "
   ],
   "metadata": {
    "id": "4Cf3akNtd-mp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "First, we present to you our toy dataset, composed of a toy training and validation sets. It is good practise to use the validation set (a representative set of the test data). This set is used to tune hyperparameters and choose a configuration for your model to ensure the best performance. Our toy sets are already tokenized and lowercased.\n",
    "\n",
    "* What is 1 ? What is 0 ?"
   ],
   "metadata": {
    "id": "rF2y5G4Ydz4t"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                               text     class\n0                              text     class\n1                    I love my job!  Positive\n2           This food is delicious.  Positive\n3          I am having a great day.  Positive\n4  I am so excited for the weekend.  Positive",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>text</td>\n      <td>class</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I love my job!</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This food is delicious.</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I am having a great day.</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I am so excited for the weekend.</td>\n      <td>Positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Since I can't import the project from github to dataSpell the same way I would do for Colab, I have to import the file manually\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://github.com/thad75/OptionAI/blob/main/TP2/sentiment_sentences.csv?raw=true\"\n",
    "df = pd.read_csv(url, names= [\"text\", \"class\"])\n",
    "#df = pd.read_csv(url, names= [\"text\", \"class\"], index_col = [0]) #index col = [0] takes off the index col that would normally be shown when reading into a csv\n",
    "df.head()# show first 5 lines\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "abMSnMwJx8bu"
   },
   "source": [
    " # Our toy sentiment analysis corpus\n",
    "train = ['i like his new book !',\n",
    "         'what a well-written novel !',\n",
    "         'i do not agree with the criticism on this short story',\n",
    "         'well done ! it was an enjoyable stage play',\n",
    "         'it was very good . send me a copy please .',\n",
    "         'the argumentation in the study is very weak',\n",
    "         'poor effort !',\n",
    "         'the descriptions could have been more detailed',\n",
    "         'i am not impressed',\n",
    "         'could have done better .',\n",
    "]\n",
    "# 1 for a postive comment and 0 for a negative comment\n",
    "train_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "# Validation set\n",
    "valid = ['i like your play', \n",
    "         'i agree with your study', \n",
    "         'what a success ! a well-written novel', \n",
    "         'not enough details . very poor', \n",
    "         'i support the criticism',\n",
    "         'could be better',\n",
    "]\n",
    "\n",
    "valid_labels = [1, 1, 1, 0, 0, 0]"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hPlpqVFYpfL"
   },
   "source": [
    "### Pre-processing\n",
    "\n",
    "* Using simple list operators, fill in the below function '...' to **tokenize the corpus.** \n",
    "\n",
    "Hint : a Token could correspond to a word, punctuation..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UQ5IL1e1GVn6"
   },
   "source": [
    "def get_tokenized_corpus(corpus):\n",
    "    tokenized_corpus = []\n",
    "\n",
    "    #######################\n",
    "    # Q: Process the corpus\n",
    "    #######################\n",
    "    for sentence in corpus:\n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '):\n",
    "            tokenized_sentence.append(token)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "\n",
    "    return tokenized_corpus\n",
    "\n",
    "get_tokenized_corpus(['i like your play'])"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[['i', 'like', 'your', 'play']]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_tokenized_corpus(corpus):\n",
    "    tokenized_corpus = []\n",
    "\n",
    "    # Process the corpus\n",
    "    for sentence in corpus:\n",
    "        tokenized_sentence = []\n",
    "\n",
    "        # Tokenize each sentence by splitting on whitespace\n",
    "        for token in sentence.split(' '):\n",
    "            tokenized_sentence.append(token)\n",
    "\n",
    "        # Add the tokenized sentence to the tokenized corpus\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "\n",
    "    return tokenized_corpus\n",
    "\n",
    "df.apply(lambda row:get_tokenized_corpus(row)).text.tolist()"
   ],
   "metadata": {
    "id": "N3XSSfdlTR4j"
   },
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[['text'],\n ['I', 'love', 'my', 'job!'],\n ['This', 'food', 'is', 'delicious.'],\n ['I', 'am', 'having', 'a', 'great', 'day.'],\n ['I', 'am', 'so', 'excited', 'for', 'the', 'weekend.'],\n ['The', 'weather', 'is', 'terrible', 'today.'],\n ['I', 'hate', 'when', 'this', 'happens.'],\n ['I', 'am', 'feeling', 'very', 'anxious.'],\n ['I', 'am', 'so', 'disappointed', 'in', 'you.'],\n ['I', 'am', 'happy', 'to', 'see', 'you.'],\n ['I', 'had', 'a', 'great', 'time', 'at', 'the', 'party.'],\n ['I', 'am', 'so', 'tired', 'of', 'this.'],\n ['I', 'am', 'feeling', 'very', 'frustrated.'],\n ['This', 'movie', 'was', 'terrible.'],\n ['I', 'am', 'feeling', 'very', 'sad.'],\n ['I', 'am', 'so', 'grateful', 'for', 'your', 'help.'],\n ['I', 'had', 'a', 'terrible', 'day', 'at', 'work.'],\n ['I', 'am', 'feeling', 'very', 'angry.'],\n ['I', 'am', 'thrilled', 'about', 'the', 'news.'],\n ['I', 'am', 'so', 'excited', 'about', 'the', 'trip.'],\n ['I', 'am', 'feeling', 'very', 'depressed.']]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyKCfY6QbO0r"
   },
   "source": [
    "### Word2index dictionary\n",
    "\n",
    "In order to easily retrieve each word in the vocabulary, we define here a method that returns a word to index dictionary. \n",
    "\n",
    "**Note** : we reserve the 0 index for the padding token `<pad>`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9bWQ3zKYKlQU"
   },
   "source": [
    "def get_word2idx(tokenized_corpus):\n",
    "  vocabulary = []\n",
    "  for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "  \n",
    "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
    "  # we reserve the 0 index for the padding token\n",
    "  word2idx['<pad>'] = 0\n",
    "  \n",
    " \n",
    "  return word2idx"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': 1,\n 'I': 2,\n 'love': 3,\n 'my': 4,\n 'job!': 5,\n 'This': 6,\n 'food': 7,\n 'is': 8,\n 'delicious.': 9,\n 'am': 10,\n 'having': 11,\n 'a': 12,\n 'great': 13,\n 'day.': 14,\n 'so': 15,\n 'excited': 16,\n 'for': 17,\n 'the': 18,\n 'weekend.': 19,\n 'The': 20,\n 'weather': 21,\n 'terrible': 22,\n 'today.': 23,\n 'hate': 24,\n 'when': 25,\n 'this': 26,\n 'happens.': 27,\n 'feeling': 28,\n 'very': 29,\n 'anxious.': 30,\n 'disappointed': 31,\n 'in': 32,\n 'you.': 33,\n 'happy': 34,\n 'to': 35,\n 'see': 36,\n 'had': 37,\n 'time': 38,\n 'at': 39,\n 'party.': 40,\n 'tired': 41,\n 'of': 42,\n 'this.': 43,\n 'frustrated.': 44,\n 'movie': 45,\n 'was': 46,\n 'terrible.': 47,\n 'sad.': 48,\n 'grateful': 49,\n 'your': 50,\n 'help.': 51,\n 'day': 52,\n 'work.': 53,\n 'angry.': 54,\n 'thrilled': 55,\n 'about': 56,\n 'news.': 57,\n 'trip.': 58,\n 'depressed.': 59,\n '<pad>': 0}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing purposes\n",
    "get_word2idx(df.apply(lambda row:get_tokenized_corpus(row)).text.tolist())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hve1MProNBvp"
   },
   "source": [
    "### Preparation of the inputs\n",
    "\n",
    "We will first train a FFN (Feed Forward Network). The first layer of our FFN will be an embedding (look-up) layer which takes as input indexes of tokens (we do not need to one-hot encode our vectors).\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Why do we need to fix the length of our input vectors (we take the maximum sentence length here) ? This process is referred to as padding. Print the padded training corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#The length of the input vectors is fixed to the maximum sentence length \n",
    "#in the corpus to ensure that all input sequences have the same length. \n",
    "#If we set the input vector length to a value smaller than the maximum sentence length, \n",
    "#then some sentences will be truncated and we will lose information."
   ],
   "metadata": {
    "id": "NdYyq6SbVG_L"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For Personal Testing Purposes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nword2idx = {\\n    'This': 0,\\n    'is': 1,\\n    'a': 2,\\n    'sentence': 3,\\n    'another': 4\\n}\\n\""
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_inputs(tokenized_corpus, word2idx, labels):\n",
    "    # we index our sentences\n",
    "    vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
    "\n",
    "tokenized_corpus = [\n",
    "    ['This', 'is', 'a', 'sentence'],\n",
    "    ['This', 'is', 'another', 'sentence']\n",
    "]\n",
    "get_word2idx(tokenized_corpus)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "word2idx = {\n",
    "    'This': 0,\n",
    "    'is': 1,\n",
    "    'a': 2,\n",
    "    'sentence': 3,\n",
    "    'another': 4\n",
    "}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_model_inputs(tokenized_corpus, word2idx, labels):\n",
    "  # we index our sentences\n",
    "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
    "\n",
    "  # Sentence lengths\n",
    "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
    "\n",
    "  # Get maximum length\n",
    "  max_len = max(sent_lengths)\n",
    "  \n",
    "  # we create a tensor of a fixed size filled with zeroes for padding\n",
    "  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long()\n",
    "\n",
    "  # we fill it with our vectorized sentences \n",
    "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
    "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
    "\n",
    "  # Label tensor\n",
    "  label_tensor = torch.FloatTensor(labels)\n",
    "\n",
    "  return sent_tensor, label_tensor\n",
    "\n",
    "# Test the functions\n",
    "train_tokenized_corpus = get_tokenized_corpus(train)\n",
    "word2idx = get_word2idx(train_tokenized_corpus)\n",
    "train_sent_tensor, train_label_tensor = get_model_inputs(train_tokenized_corpus, word2idx, train_labels)\n"
   ],
   "metadata": {
    "id": "0bOOUU9iWi6g"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data visualization \n"
   ],
   "metadata": {
    "id": "JAUpE-Yag74O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Existing CSV File\n",
    "\n",
    "* Go to your workspace by clicking on the folder icon on the left of your notebook and click on the sentimen_sentences.csv file\n",
    "* What do you see ? Explain the Dataset\n",
    "\n",
    "In order to have insight about what embeddings look like, use https://huggingface.co/spaces/mohitmayank/sentenceviz with the sentiment_sentences CSV file available on the github or in your google colab workspace. You just have to download it and load it in the website to vizualise your embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Q:** \n",
    "* **What changes can you see between the different embeddings visualization techniques ?**\n",
    "* **What can you infer from these differences ?**"
   ],
   "metadata": {
    "id": "ZCtqBHwFf-7y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating our own corpus\n",
    "\n",
    "Now you can try to visualize embeddings of our corpus using the csv module to convert the list of word into a CSV file matching the one you already used. \n",
    "\n",
    "* Create a CSV File based on our corpus and perform the same data visualization\n",
    "---\n",
    "\n",
    "**Q: What differences do you notice ?**"
   ],
   "metadata": {
    "id": "86iDk7yJyskB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#The CSV file contains different comments to which\n",
    "#we assign labels \"Positive\" for a positive comment and \"Negative\" for a \n",
    "#negative one as the 1 and 0s in the first part\n",
    "#the distance between 2 sentences reprents the similarity\n",
    "#degree between these 2 statements "
   ],
   "metadata": {
    "id": "nmlpwaLSX2Il"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the name of the CSV file\n",
    "filename = \"corpus_sentences.csv\"\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(filename, 'w', newline='') as file:\n",
    "\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"text\",\"class\"])\n",
    "\n",
    "    # Write sentences to the CSV file\n",
    "    for sentence in train:\n",
    "        writer.writerow([sentence,train_labels[train.index(sentence)]]) # Hint : https://docs.python.org/3/library/csv.html"
   ],
   "metadata": {
    "id": "IfTE6XDvz2_k"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGg6CtALe8Va"
   },
   "source": [
    "## Model 1 : the Feed-Forward Network\n",
    "\n",
    "We will start by building a very simple feed-forward neural network (FFNN).\n",
    "Our FFNN class is a sub-class of `nn.Module`. Within the `__init__` method, we define the layers of the module:\n",
    "\n",
    "- Our first layer is an embedding layer (look-up layer). This layer could be initialized with pre-trained embeddings (as we will see at the end of this lab) or could be trained together with other layers.\n",
    " \n",
    "- The next layer is a fully connected layer followed by a ReLU activation.\n",
    "\n",
    "- Finally, the last linear layer is the output layer for the classification task.\n",
    "\n",
    "The `forward()` method is called when we feed data into our model. Please note that the output dimension of each layer is the input dimension for the next one.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Q: Implement the averaging of embeddings in the `forward()` method of the class below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4LWut1gtXGQN"
   },
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes):  \n",
    "        super(FFNN, self).__init__()\n",
    "        \n",
    "        # embedding (lookup layer) layer\n",
    "        # padding_idx argument makes sure that the 0-th token in the vocabulary\n",
    "        # is used for padding purposes i.e. its embedding will be a 0-vector\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # hidden layer\n",
    "        # TODO : What is the input size of the first Linear Layer\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # activation\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # output layer\n",
    "        # TODO : What is the output size of the last Linear Layer\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim,num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, max_sent_len)\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        # `embedding` has shape (batch size, max_sent_len, embedding dim)\n",
    "\n",
    "        ########################################################################\n",
    "        # Q: Compute the average embeddings of shape (batch_size, embedding_dim)\n",
    "        ########################################################################\n",
    "        # Implement averaging that ignores padding (average using actual sentence lengths).\n",
    "        # How this effect the result?\n",
    "        \n",
    "        sent_lens = x.ne(0).sum(dim=1).unsqueeze(1).float()\n",
    "        # x.ne(0) returns a tensor of the same size as x with 1s where the\n",
    "        # values of x are non-zero and 0s where they are zero\n",
    "        # .sum(dim=1) returns a tensor with the sum of the values along the second axis\n",
    "        # .unsqueeze(1) adds an extra dimension to the tensor at position 1\n",
    "        # (the size of this dimension is 1)\n",
    "        # .float() converts the tensor to a tensor of floats\n",
    "\n",
    "        averaged = embedded.sum(dim=1) / sent_lens\n",
    "        # embedded.sum(dim=1) returns a tensor of shape (batch_size, embedding_dim)\n",
    "        # containing the sum of the embeddings for each sentence\n",
    "        # / sent_lens broadcasts the tensor of sentence lengths to the same shape as\n",
    "        # embedded.sum(dim=1), so we can divide each sum by the length of the corresponding\n",
    "        # sentence\n",
    "\n",
    "        out = self.fc1(averaged)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcAI5BtaQMFh"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "In this section, we will define the hyperparameters of our model, the loss function, the optimizer and perform a number of training epochs over our toy training corpus.\n",
    "\n",
    "We will use the **Stochastic gradient descent (SGD)** optimizer. The learning rate hyperparameter of the optimizer controls how the weights are adjusted with respect to the loss gradient. The lower the value, the more fine-grained are weight updates.\n",
    "\n",
    "**Note that** it is a common practise to perform training using mini-batches (sets of training instances seen by the model during weight update step). In this case, the epoch loss is defined as the loss averaged across the mini-batches. Since our corpus is very small, we train on the whole training set without batching.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Why is the number of output classes is equal to 1 for binary classification?**\n",
    "\n",
    "\n",
    "**Q: Try to modify the learning rate (which is initially set to 0.5 below) in the range $[0.0001, 0.5]$. How does the loss react to these changes?**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wrtOwTUsyArb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ce3ef800-42b7-4a51-dccb-8805674de63a"
   },
   "source": [
    "# we will train for N epochs (The model will see the corpus N times)\n",
    "EPOCHS = 10\n",
    "\n",
    "# Learning rate is initially set to 0.5\n",
    "LRATE = 0.5\n",
    "\n",
    "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# dimensionality of the output of the second hidden layer\n",
    "HIDDEN_DIM = 50\n",
    "\n",
    "# the output dimension is the number of classes, 1 for binary classification\n",
    "# TODO : What is the number of classes. Hint : What task are we performing ?\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# Construct the model\n",
    "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
    "\n",
    "# Print the model\n",
    "print(model)\n",
    "\n",
    "# TODO : initialize your optimizer, we use the stochastic gradient descent (SGD) optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
    "\n",
    "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
    "# Recall that we did not apply any activation to our output layer, hence we need\n",
    "# to make our outputs look like probabilities.\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Input and label tensors\n",
    "feature = train_sent_tensor\n",
    "target = train_label_tensor\n",
    "\n",
    "################\n",
    "# Start training\n",
    "################\n",
    "\n",
    "# to ensure the dropout (explained later) is \"turned on\" while training\n",
    "# good practice to include even if do not use here\n",
    "model.train()\n",
    "\n",
    "print(f'Will train for {EPOCHS} epochs')\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "  # we zero the gradients as they are not removed automatically\n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  # TODO : Send the data to your model\n",
    "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
    "  # and we need to remove the dimension of size 1\n",
    "  predictions = model(feature).squeeze(1)\n",
    "\n",
    "  # TODO : Compute the loss between the predictions and the targets\n",
    "  loss = loss_fn(predictions, target)\n",
    "  train_loss = loss.item()\n",
    "\n",
    "  # calculate the gradient of each parameter\n",
    "  loss.backward()\n",
    "\n",
    "  # update the parameters using the gradients and optimizer algorithm \n",
    "  optimizer.step()\n",
    "  \n",
    "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f}')"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN(\n",
      "  (embedding): Embedding(52, 50, padding_idx=0)\n",
      "  (fc1): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n",
      "Will train for 10 epochs\n",
      "| Epoch: 01 | Train Loss: 0.707\n",
      "| Epoch: 02 | Train Loss: 0.670\n",
      "| Epoch: 03 | Train Loss: 0.634\n",
      "| Epoch: 04 | Train Loss: 0.597\n",
      "| Epoch: 05 | Train Loss: 0.555\n",
      "| Epoch: 06 | Train Loss: 0.509\n",
      "| Epoch: 07 | Train Loss: 0.460\n",
      "| Epoch: 08 | Train Loss: 0.410\n",
      "| Epoch: 09 | Train Loss: 0.361\n",
      "| Epoch: 10 | Train Loss: 0.316\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwY-k6x0xIkl"
   },
   "source": [
    "### Measuring the accuracy\n",
    "\n",
    "In addition to measuring the loss, we can also evaluate the actual classification performance of our model. (In the case of training with mini-batches, the epoch accuracy is defined as the accuracy averaged across the mini-batches.)\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Fill in the below function so that it computes the accuracy of the model. Once you are done, improve the loop so that it also prints the training accuracy after each epoch.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0R23R0XjuPTn"
   },
   "source": [
    "def accuracy(output, target):\n",
    "  #####################################\n",
    "  # Q: Return the accuracy of the model\n",
    "  #####################################)\n",
    "\n",
    "  output = torch.round(torch.sigmoid(output))\n",
    "  correct = (output == target).float()\n",
    "  acc = correct.mean()\n",
    "  return acc\n"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9eP2s5MyyrNO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ae7c6dda-11ca-483f-dae6-f8ac0468fb57"
   },
   "source": [
    "# we will train for N epochs (The model will see the corpus N times)\n",
    "EPOCHS = 10\n",
    "\n",
    "# Learning rate is initially set to 0.5\n",
    "LRATE = 0.5\n",
    "\n",
    "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# dimensionality of the output of the second hidden layer\n",
    "HIDDEN_DIM = 50\n",
    "\n",
    "# the output dimension is the number of classes, 1 for binary classification\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# Construct the model\n",
    "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
    "\n",
    "# Print the model\n",
    "print(model)\n",
    "\n",
    "# we use the stochastic gradient descent (SGD) optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
    "\n",
    "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
    "# Recall that we did not apply any activation to our output layer, hence we need\n",
    "# to make our outputs look like probabilities.\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Input and label tensors\n",
    "feature = train_sent_tensor\n",
    "target = train_label_tensor\n",
    "\n",
    "# to ensure the dropout (explained later) is \"turned on\" while training\n",
    "# good practice to include even if do not use here\n",
    "model.train()\n",
    "\n",
    "################\n",
    "# Start training\n",
    "################\n",
    "print(f'Will train for {EPOCHS} epochs')\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "  # we zero the gradients as they are not removed automatically\n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
    "  # and we need to remove the dimension of size 1\n",
    "  predictions = model(feature).squeeze(1)\n",
    "  \n",
    "\n",
    "  # Compute the loss\n",
    "  loss = loss_fn(predictions, target)\n",
    "  train_loss = loss.item()\n",
    "  \n",
    "  #####################\n",
    "  # Q: Compute accuracy\n",
    "  #####################\n",
    "  train_acc = accuracy(predictions,target)\n",
    "\n",
    "  # calculate the gradient of each parameter\n",
    "  loss.backward()\n",
    "\n",
    "  # update the parameters using the gradients and optimizer algorithm \n",
    "  optimizer.step()\n",
    "\n",
    "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN(\n",
      "  (embedding): Embedding(52, 50, padding_idx=0)\n",
      "  (fc1): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n",
      "Will train for 10 epochs\n",
      "| Epoch: 01 | Train Loss: 0.678 | Train Acc: 60.00%\n",
      "| Epoch: 02 | Train Loss: 0.649 | Train Acc: 90.00%\n",
      "| Epoch: 03 | Train Loss: 0.621 | Train Acc: 90.00%\n",
      "| Epoch: 04 | Train Loss: 0.591 | Train Acc: 100.00%\n",
      "| Epoch: 05 | Train Loss: 0.558 | Train Acc: 100.00%\n",
      "| Epoch: 06 | Train Loss: 0.522 | Train Acc: 100.00%\n",
      "| Epoch: 07 | Train Loss: 0.482 | Train Acc: 100.00%\n",
      "| Epoch: 08 | Train Loss: 0.440 | Train Acc: 100.00%\n",
      "| Epoch: 09 | Train Loss: 0.397 | Train Acc: 100.00%\n",
      "| Epoch: 10 | Train Loss: 0.355 | Train Acc: 100.00%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2UhMGlMq3Re"
   },
   "source": [
    "### Hyperparameter tuning on the validation set\n",
    "\n",
    "You should now apply the previous pre-processing and input preparation procedures to the validation set as well.\n",
    "\n",
    "---\n",
    "\n",
    "**Q:** \n",
    "* **Should we re-use the word to index dictionary we created before?**\n",
    "* **Why?**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i9_FERooxUQR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5b044c1a-16bd-41d4-bee0-7aa969eab0a4"
   },
   "source": [
    "###############################################\n",
    "# Q: Prepare the validation corpus and labels #\n",
    "###############################################\n",
    "tokenized_corpus_valid = get_tokenized_corpus(valid)\n",
    "word2idx_valid = get_word2idx(tokenized_corpus_valid)\n",
    "valid_sent_tensor, valid_label_tensor = get_model_inputs(tokenized_corpus_valid, word2idx_valid, valid_labels)\n",
    "print(valid_sent_tensor)"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  0,  0,  0],\n",
      "        [ 1,  5,  6,  3,  7,  0,  0],\n",
      "        [ 8,  9, 10, 11,  9, 12, 13],\n",
      "        [14, 15, 16, 17, 18, 19,  0],\n",
      "        [ 1, 20, 21, 22,  0,  0,  0],\n",
      "        [23, 24, 25,  0,  0,  0,  0]])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUlXfydh1Uxv"
   },
   "source": [
    "**Q: Try to modify the learning rate and the number of epochs now. How will the validation loss and accuracy react to those changes?**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rcKPET9BHb-7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ba4dfd2d-b705-422f-d69f-371253bc980d"
   },
   "source": [
    "# we will train for N epochs (The model will see the corpus N times)\n",
    "EPOCHS = 10\n",
    "\n",
    "# Learning rate is initially set to 0.5\n",
    "LRATE = 0.5\n",
    "\n",
    "# we define our embedding dimension (dimensionality of the output of the first layer)\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# dimensionality of the output of the second hidden layer\n",
    "HIDDEN_DIM = 50\n",
    "\n",
    "# the output dimension is the number of classes, 1 for binary classification\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# Construct the model\n",
    "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), OUTPUT_DIM)\n",
    "\n",
    "# we use the stochastic gradient descent (SGD) optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
    "\n",
    "# we use the binary cross-entropy loss with sigmoid (applied to logits) \n",
    "# Recall that we did not apply any activation to our output layer, hence we need\n",
    "# to make our outputs look like probabilities.\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Input and label tensors for training\n",
    "feature_train = train_sent_tensor\n",
    "target_train = train_label_tensor\n",
    "\n",
    "# Input and label tensors for validation\n",
    "feature_valid = valid_sent_tensor\n",
    "target_valid = valid_label_tensor\n",
    "\n",
    "# to ensure the dropout (explained later) is \"turned on\" while training\n",
    "# good practice to include even if do not use here\n",
    "model.train()\n",
    "################\n",
    "# Start training\n",
    "################\n",
    "print(f'Will train for {EPOCHS} epochs')\n",
    "for epoch in range(1, EPOCHS + 1):  \n",
    "  # we zero the gradients as they are not removed automatically\n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  # squeeze is needed as the predictions will have the shape (batch size, 1)\n",
    "  # and we need to remove the dimension of size 1\n",
    "  predictions = model(feature_train).squeeze(1)\n",
    "\n",
    "  # Compute the loss\n",
    "  loss = loss_fn(predictions, target_train)\n",
    "  train_loss = loss.item()\n",
    "\n",
    "  # Compute training accuracy\n",
    "  train_acc = accuracy(predictions, target_train)\n",
    "\n",
    "  # calculate the gradient of each parameter\n",
    "  loss.backward()\n",
    "\n",
    "  # update the parameters using the gradients and optimizer algorithm \n",
    "  optimizer.step()\n",
    "  \n",
    "  # this puts the model in \"evaluation mode\" (turns off dropout and batch normalization)\n",
    "  # good practise to include even if we do not use them right now\n",
    "  model.eval()\n",
    "\n",
    "  # we do not compute gradients within this block, i.e. no training\n",
    "  with torch.no_grad():\n",
    "    predictions_valid = model(feature_valid).squeeze(1)\n",
    "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
    "    valid_acc = accuracy(predictions_valid, target_valid)\n",
    "  \n",
    "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train for 10 epochs\n",
      "| Epoch: 01 | Train Loss: 0.705 | Train Acc:  20.00% | Val. Loss: 0.709 | Val. Acc:  16.67% |\n",
      "| Epoch: 02 | Train Loss: 0.673 | Train Acc:  80.00% | Val. Loss: 0.697 | Val. Acc:  50.00% |\n",
      "| Epoch: 03 | Train Loss: 0.645 | Train Acc:  80.00% | Val. Loss: 0.685 | Val. Acc:  66.67% |\n",
      "| Epoch: 04 | Train Loss: 0.617 | Train Acc:  80.00% | Val. Loss: 0.672 | Val. Acc:  66.67% |\n",
      "| Epoch: 05 | Train Loss: 0.585 | Train Acc:  90.00% | Val. Loss: 0.659 | Val. Acc:  66.67% |\n",
      "| Epoch: 06 | Train Loss: 0.549 | Train Acc: 100.00% | Val. Loss: 0.645 | Val. Acc:  66.67% |\n",
      "| Epoch: 07 | Train Loss: 0.510 | Train Acc: 100.00% | Val. Loss: 0.633 | Val. Acc:  66.67% |\n",
      "| Epoch: 08 | Train Loss: 0.467 | Train Acc: 100.00% | Val. Loss: 0.624 | Val. Acc:  66.67% |\n",
      "| Epoch: 09 | Train Loss: 0.421 | Train Acc: 100.00% | Val. Loss: 0.615 | Val. Acc:  66.67% |\n",
      "| Epoch: 10 | Train Loss: 0.375 | Train Acc: 100.00% | Val. Loss: 0.616 | Val. Acc:  66.67% |\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npzWXeQATPs4"
   },
   "source": [
    "### Testing the model\n",
    "\n",
    "Now let us test our trained model. We define a small test set below. First, apply the data preparation procedures to this test set as you did for the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5Cx2eJi1a8R6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "42dd97e5-1ad6-4e44-f514-b3d2765cf771"
   },
   "source": [
    "test = ['i really do not like your short story', \n",
    "        'well done', \n",
    "        'good results for a study !',\n",
    "        'amazing effort', \n",
    "        'your effort is poor !', \n",
    "        'not impressed'   \n",
    "]\n",
    "\n",
    "test_labels = [0, 1, 1, 1, 0, 0]\n",
    "\n",
    "#########################################\n",
    "# Q: Prepare the test corpus and labels #\n",
    "tokenized_corpus_test = get_tokenized_corpus(test)\n",
    "word2idx_test = get_word2idx(tokenized_corpus_test)\n",
    "test_sent_tensor, test_label_tensor = get_model_inputs(tokenized_corpus_test, word2idx_test, test_labels)\n",
    "print(test_sent_tensor)"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8],\n",
      "        [ 9, 10,  0,  0,  0,  0,  0,  0],\n",
      "        [11, 12, 13, 14, 15, 16,  0,  0],\n",
      "        [17, 18,  0,  0,  0,  0,  0,  0],\n",
      "        [ 6, 18, 19, 20, 16,  0,  0,  0],\n",
      "        [ 4, 21,  0,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0A1Iu-p4FUL"
   },
   "source": [
    "**Q: Fill in the below function for the computation of F-measure. Once done, complete the missing lines in the final evaluation part.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ixhvaYkW_SfX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "56a1e8a0-a89b-4aa6-d414-bb38f8093e8b"
   },
   "source": [
    "def f_measure(output, gold):\n",
    "  ############################################\n",
    "  # Q: Compute precision, recall and f-measure \n",
    "  ############################################\n",
    "  pred = torch.round(torch.sigmoid(output))\n",
    "  pred = pred.detach().cpu().numpy()\n",
    "     \n",
    "  test_pos_preds = np.sum(pred)\n",
    "  test_pos_real = np.sum(gold)\n",
    "  \n",
    "  #On garde dans correct les reponses similaires\n",
    "  correct = (np.logical_xor(pred, gold)).astype(int)\n",
    "  correct = np.sum(1-correct)\n",
    "  \n",
    "  precision = correct / test_pos_preds\n",
    "  recall = correct / test_pos_real\n",
    "  \n",
    "  fscore = (2.0 * precision * recall) / (precision+ recall)\n",
    "\n",
    "  # Print them\n",
    "  print(f\"     Recall: {recall:.2f}, Precision: {precision:.2f}, F-measure: {fscore:.2f}\")\n",
    "  \n",
    "\n",
    "####\n",
    "\n",
    "model.eval()\n",
    "\n",
    "feature_test = test_sent_tensor\n",
    "target_test = test_label_tensor\n",
    "\n",
    "with torch.no_grad():\n",
    "  ####################################################################\n",
    "  # Q: Get predictions for the test set, compute the loss and accuracy\n",
    "  ####################################################################\n",
    "  predictions = model(feature_test).squeeze(1)\n",
    "  test_loss = loss_fn(predictions, target_test).item()\n",
    "  test_acc = accuracy(predictions, target_test)\n",
    "\n",
    "  # Print\n",
    "  print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "  f_measure(predictions, test_labels)  "
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.762 | Test Acc: 50.00%\n",
      "     Recall: 1.00, Precision: 0.50, F-measure: 0.67\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofMn0IjZ3lUr"
   },
   "source": [
    "**Q:  Are the resulting evaluations different ? How do you interpret those differences? Print the predictions.**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2znwTJeU3UT"
   },
   "source": [
    "## Model 2 : the Convolutional Neural Network (CNN)\n",
    "\n",
    "We will implement a model inspired by the CNN model as described in [Convolutional Neural Networks for Sentence Classification (Kim, 2014)](https://arxiv.org/abs/1408.5882).\n",
    "\n",
    "Similar to the FFNN model, we start with an embedding layer. We implement the convolutional layer with the help of `nn.Conv2d` and use the ReLU activation after it. The above-mentioned paper, being inspired by the convolution for images, applies a 2-dimensional convolution: a (window size, embedding dimension) filter. It covers `n` sequential words, taking embedding dimensions as the width. We then pass the tensors through a **max pooling layer**.\n",
    "\n",
    "The **max pooling layer** is typically followed by a **dropout** layer. The latter sets a random set of activations in the max-pooling layer to zero. This prevents the network from learning to rely on specific weights and helps to prevent overfitting. Note that the dropout layer is only used during training, and not during test time.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Study the shapes of outputs coming from convolution and max pooling layers. What is the shape of the max pooling layer output?**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NUPETZaOgvgB"
   },
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, out_channels, window_size, output_dim, dropout):\n",
    "    super(CNN, self).__init__()\n",
    "    \n",
    "    # Create the embedding layer as usual\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "    \n",
    "    # in_channels -- 1 text channel\n",
    "    # out_channels -- the number of output channels\n",
    "    # kernel_size is (window size x embedding dim)\n",
    "    self.conv = nn.Conv2d(\n",
    "      in_channels=1, out_channels=out_channels,\n",
    "      kernel_size=(window_size, embedding_dim))\n",
    "    \n",
    "    # the dropout layer\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # the output layer\n",
    "    self.fc = nn.Linear(out_channels, output_dim)\n",
    "        \n",
    "  def forward(self, x):\n",
    "    # x -> (batch size, max_sent_length)\n",
    "    \n",
    "    embedded = self.embedding(x)\n",
    "    # embedded -> (batch size, max_sent_length, embedding_dim)\n",
    "    \n",
    "    # images have 3 RGB channels \n",
    "    # for the text we add 1 channel\n",
    "    embedded = embedded.unsqueeze(1)\n",
    "    # embedded -> (batch size, 1, max_sent_length, embedding dim)\n",
    "\n",
    "    # Compute the feature maps      \n",
    "    feature_maps = self.conv(embedded)\n",
    "\n",
    "    ##########################################\n",
    "    # Q: What is the shape of `feature_maps` ?\n",
    "    ##########################################\n",
    "    \n",
    "    feature_maps = feature_maps.squeeze(3)\n",
    "    \n",
    "    ##########################################\n",
    "    # Q: why do we remove 1 dimension here?\n",
    "    ##########################################\n",
    "    \n",
    "    # Apply ReLU\n",
    "    feature_maps = F.relu(feature_maps)\n",
    "    \n",
    "    # Apply the max pooling layer\n",
    "    pooled = F.max_pool1d(feature_maps, feature_maps.shape[2])\n",
    "    \n",
    "    pooled = pooled.squeeze(2)\n",
    "\n",
    "    ####################################\n",
    "    # Q: What is the shape of `pooled` ?\n",
    "    ####################################\n",
    "    \n",
    "    dropped = self.dropout(pooled)\n",
    "    preds = self.fc(dropped)\n",
    "    \n",
    "    return preds"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UZroMvLtFVv"
   },
   "source": [
    "### Training and testing the CNN\n",
    "\n",
    "Here we will define the CNN-specific hyperparameters and perform the network training and testing. **Note that** the learning rate is initially set to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Lq1lcWbqRwNY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7b59a21d-d21d-42d7-ec35-a190ad7a6b50"
   },
   "source": [
    "fix_seed()\n",
    "\n",
    "EPOCHS = 10\n",
    "LRATE = 0.1\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# the hyperparameters specific to CNN\n",
    "# we define the number of filters\n",
    "N_OUT_CHANNELS = 100\n",
    "\n",
    "# we define the window size\n",
    "WINDOW_SIZE = 1\n",
    "\n",
    "# we apply the dropout with the probability 0.2\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Construct the model\n",
    "model = CNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "feature_train = train_sent_tensor\n",
    "target_train = train_label_tensor\n",
    "\n",
    "feature_valid = valid_sent_tensor\n",
    "target_valid = valid_label_tensor\n",
    "\n",
    "feature_test = test_sent_tensor\n",
    "target_test = test_label_tensor\n",
    "\n",
    "################\n",
    "# Start training\n",
    "################\n",
    "print(f'Will train for {EPOCHS} epochs')\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "  model.train()\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  predictions = model(feature_train).squeeze(1)\n",
    "  loss = loss_fn(predictions, target_train)\n",
    "  train_loss = loss.item()\n",
    "  train_acc = accuracy(predictions, target_train)\n",
    "\n",
    "  loss.backward()\n",
    "\n",
    "  optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    predictions_valid = model(feature_valid).squeeze(1)\n",
    "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
    "    valid_acc = accuracy(predictions_valid, target_valid)\n",
    "  \n",
    "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')\n",
    "\n",
    "\n",
    "## Finally, test on the test set\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(feature_test).squeeze(1)\n",
    "    loss = loss_fn(predictions, target_test)\n",
    "    acc = accuracy(predictions, target_test)\n",
    "    print(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "    f_measure(predictions, test_labels)"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train for 10 epochs\n",
      "| Epoch: 01 | Train Loss: 0.766 | Train Acc:  50.00% | Val. Loss: 0.711 | Val. Acc:  50.00% |\n",
      "| Epoch: 02 | Train Loss: 0.672 | Train Acc:  50.00% | Val. Loss: 0.679 | Val. Acc:  50.00% |\n",
      "| Epoch: 03 | Train Loss: 0.603 | Train Acc:  60.00% | Val. Loss: 0.663 | Val. Acc:  50.00% |\n",
      "| Epoch: 04 | Train Loss: 0.509 | Train Acc:  80.00% | Val. Loss: 0.649 | Val. Acc:  66.67% |\n",
      "| Epoch: 05 | Train Loss: 0.534 | Train Acc:  70.00% | Val. Loss: 0.647 | Val. Acc:  50.00% |\n",
      "| Epoch: 06 | Train Loss: 0.426 | Train Acc: 100.00% | Val. Loss: 0.684 | Val. Acc:  50.00% |\n",
      "| Epoch: 07 | Train Loss: 0.467 | Train Acc:  80.00% | Val. Loss: 0.640 | Val. Acc:  66.67% |\n",
      "| Epoch: 08 | Train Loss: 0.445 | Train Acc: 100.00% | Val. Loss: 0.675 | Val. Acc:  50.00% |\n",
      "| Epoch: 09 | Train Loss: 0.372 | Train Acc:  90.00% | Val. Loss: 0.663 | Val. Acc:  50.00% |\n",
      "| Epoch: 10 | Train Loss: 0.316 | Train Acc: 100.00% | Val. Loss: 0.671 | Val. Acc:  50.00% |\n",
      "Test Loss: 0.932 | Test Acc: 50.00%\n",
      "     Recall: 1.00, Precision: 0.50, F-measure: 0.67\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5si3n_wf-ImU"
   },
   "source": [
    " **Q: Is the performance of CNN different from the performance of FFNN? Output predictions.**\n",
    " \n",
    "**Q: Is padding necessary for CNN inputs? What is the role of the window size?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4VCX4XSqvlu"
   },
   "source": [
    "### Initializing CNN with pre-trained representations\n",
    "\n",
    "The work [Convolutional Neural Networks for Sentence Classification (Kim, 2014)](https://arxiv.org/abs/1408.5882) also investigates the exploitation of pre-trained embeddings and demonstrates the efficiency of using them.\n",
    "\n",
    "First, download the embeddings and unzip them below:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PgvTCi68lOGn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5f9ec92a-ad56-4635-b3a2-6a7a03ce032e"
   },
   "source": [
    "\"\"\"\n",
    "wget not working. urlretrieve is a good alternative\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "filename = \"glove.6B.zip\"\n",
    "\n",
    "\"\"\"\n",
    " Shows a progress bar for the task at hand(urlretrieve)\n",
    " @:param unit: progression Unit. Set to Bytes\n",
    " @:param unit_scale: number of iteration. Set to be scaled automatically\n",
    " @:param Minimum progress display update interval in iterations. Set to 1 in case download speed is erratic\n",
    " @:param Prefix of the progress bar\n",
    "\"\"\"\n",
    "with tqdm(unit='B', unit_scale=True, miniters=1, desc=filename) as progressBar: # getting a progress bar because I wasn't getting one when using the urlretrieve command\n",
    "    urllib.request.urlretrieve(url, filename, reporthook=lambda blocks, block_size, total_size: progressBar.update(block_size))\n",
    "\n",
    "#!curl http://nlp.stanford.edu/data/glove.6B.zip\n"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "glove.6B.zip: 90.5MB [00:15, 5.30MB/s]"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kq8ou3Y0CSQh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5d6e6b54-ed62-41bb-bab3-8120aa6b5dba"
   },
   "source": [
    "# Unzip the file: 4 different embedding sizes are provided\n",
    "\n",
    "import sys\n",
    "if sys.executable == 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\BookEx\\\\python.exe':\n",
    "    !{sys.executable} -m pip install zipfile\n",
    "\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "fileName = \"glove.6B.zip\"\n",
    "\n",
    "\n",
    "# Open the zip file in read mode\n",
    "with zipfile.ZipFile(fileName, \"r\") as zipRef:\n",
    "    # Get the list of all files in the zip archive\n",
    "    files = zipRef.namelist()\n",
    "    # Loop through each file and extract it with a progress bar\n",
    "    for file in tqdm(files, desc=\"Extracting files\"):\n",
    "        zipRef.extract(file)"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting files: 100%|██████████| 4/4 [00:11<00:00,  2.92s/it]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "afclMKCYLYT9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "23d13cdc-033f-4317-e632-b2814ad82ffc"
   },
   "source": [
    "# Once again, head is not a windows command so here is the equivalent :(\n",
    "# Check the file format\n",
    "#!head -n10 glove.6B.50d.txt\n",
    "\n",
    "\n",
    "with open(\"glove.6B.50d.txt\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "        if i >= 50:\n",
    "            break\n",
    "        print(line.strip())\n",
    "\n",
    "\n"
   ],
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
      ". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n",
      "of 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\n",
      "to 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\n",
      "and 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\n",
      "in 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\n",
      "a 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n",
      "\" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n",
      "'s 0.23727 0.40478 -0.20547 0.58805 0.65533 0.32867 -0.81964 -0.23236 0.27428 0.24265 0.054992 0.16296 -1.2555 -0.086437 0.44536 0.096561 -0.16519 0.058378 -0.38598 0.086977 0.0033869 0.55095 -0.77697 -0.62096 0.092948 -2.5685 -0.67739 0.10151 -0.48643 -0.057805 3.1859 -0.017554 -0.16138 0.055486 -0.25885 -0.33938 -0.19928 0.26049 0.10478 -0.55934 -0.12342 0.65961 -0.51802 -0.82995 -0.082739 0.28155 -0.423 -0.27378 -0.007901 -0.030231\n",
      "for 0.15272 0.36181 -0.22168 0.066051 0.13029 0.37075 -0.75874 -0.44722 0.22563 0.10208 0.054225 0.13494 -0.43052 -0.2134 0.56139 -0.21445 0.077974 0.10137 -0.51306 -0.40295 0.40639 0.23309 0.20696 -0.12668 -0.50634 -1.7131 0.077183 -0.39138 -0.10594 -0.23743 3.9552 0.66596 -0.61841 -0.3268 0.37021 0.25764 0.38977 0.27121 0.043024 -0.34322 0.020339 0.2142 0.044097 0.14003 -0.20079 0.074794 -0.36076 0.43382 -0.084617 0.1214\n",
      "- -0.16768 1.2151 0.49515 0.26836 -0.4585 -0.23311 -0.52822 -1.3557 0.16098 0.37691 -0.92702 -0.43904 -1.0634 1.028 0.0053943 0.04153 -0.018638 -0.55451 0.026166 0.28066 -0.66245 0.23435 0.2451 0.025668 -1.0869 -2.844 -0.51272 0.27286 0.0071502 0.033984 3.9084 0.52766 -0.66899 1.8238 0.43436 -0.30084 -0.26996 0.4394 0.69956 0.14885 0.029453 1.4888 0.52361 0.099354 1.2515 0.099381 -0.079261 -0.30862 0.30893 0.11023\n",
      "that 0.88387 -0.14199 0.13566 0.098682 0.51218 0.49138 -0.47155 -0.30742 0.01963 0.12686 0.073524 0.35836 -0.60874 -0.18676 0.78935 0.54534 0.1106 -0.2923 0.059041 -0.69551 -0.18804 0.19455 0.32269 -0.49981 0.306 -2.3902 -0.60749 0.37107 0.078912 -0.23896 3.839 -0.20355 -0.35613 -0.69185 -0.17497 -0.35323 0.10598 -0.039303 0.015701 0.038279 -0.35283 0.44882 -0.16534 0.31579 0.14963 -0.071277 -0.53506 0.52711 -0.20148 0.0095952\n",
      "on 0.30045 0.25006 -0.16692 0.1923 0.026921 -0.079486 -0.91383 -0.1974 -0.053413 -0.40846 -0.26844 -0.28212 -0.5 0.1221 0.3903 0.17797 -0.4429 -0.40478 -0.9505 -0.16897 0.77793 0.33525 0.3346 -0.1754 -0.12017 -1.7861 0.29241 0.55933 0.029982 -0.32417 3.9297 0.1088 -0.57335 -0.17842 0.0041748 -0.16309 0.45077 -0.16123 -0.17311 -0.087889 -0.089032 0.062001 -0.19946 -0.38863 -0.18232 0.060751 0.098603 -0.07131 0.23052 -0.51939\n",
      "is 0.6185 0.64254 -0.46552 0.3757 0.74838 0.53739 0.0022239 -0.60577 0.26408 0.11703 0.43722 0.20092 -0.057859 -0.34589 0.21664 0.58573 0.53919 0.6949 -0.15618 0.05583 -0.60515 -0.28997 -0.025594 0.55593 0.25356 -1.9612 -0.51381 0.69096 0.066246 -0.054224 3.7871 -0.77403 -0.12689 -0.51465 0.066705 -0.32933 0.13483 0.19049 0.13812 -0.21503 -0.016573 0.312 -0.33189 -0.026001 -0.38203 0.19403 -0.12466 -0.27557 0.30899 0.48497\n",
      "was 0.086888 -0.19416 -0.24267 -0.33391 0.56731 0.39783 -0.97809 0.03159 -0.61469 -0.31406 0.56145 0.12886 -0.84193 -0.46992 0.47097 0.023012 -0.59609 0.22291 -1.1614 0.3865 0.067412 0.44883 0.17394 -0.53574 0.17909 -2.1647 -0.12827 0.29036 -0.15061 0.35242 3.124 -0.90085 -0.02567 -0.41709 0.40565 -0.22703 0.76829 0.60982 0.070068 -0.13271 -0.1201 0.096132 -0.43998 -0.48531 -0.5188 -0.3077 -0.75028 -0.77 0.3945 -0.16937\n",
      "said 0.38973 -0.2121 0.51837 0.80136 1.0336 -0.27784 -0.84525 -0.25333 0.12586 -0.90342 0.24975 0.22022 -1.2053 -0.53771 1.0446 0.62778 0.39704 -0.15812 0.38102 -0.54674 -0.44009 1.0976 0.013069 -0.89971 0.41226 -2.2309 0.28997 0.32175 -0.72738 -0.092244 3.028 -0.062599 0.038329 0.0072918 -0.35388 -0.92256 0.097932 0.10068 1.2116 0.88233 -0.46297 1.3186 0.32705 -0.73446 0.89301 -0.45324 -1.2698 0.86119 0.1415 1.2018\n",
      "with 0.25616 0.43694 -0.11889 0.20345 0.41959 0.85863 -0.60344 -0.31835 -0.6718 0.003984 -0.075159 0.11043 -0.73534 0.27436 0.054015 -0.23828 -0.13767 0.011573 -0.46623 -0.55233 0.083317 0.55938 0.51903 -0.27065 -0.28211 -1.3918 0.17498 0.26586 0.061449 -0.273 3.9032 0.38169 -0.056009 -0.004425 0.24033 0.30675 -0.12638 0.33436 0.075485 -0.036218 0.13691 0.37762 -0.12159 -0.13808 0.19505 0.22793 -0.17304 -0.07573 -0.25868 -0.39339\n",
      "he -0.20092 -0.060271 -0.61766 -0.8444 0.5781 0.14671 -0.86098 0.6705 -0.86556 -0.18234 0.15856 0.45814 -1.0163 -0.35874 0.73869 -0.24048 -0.33893 0.25742 -0.78192 0.083528 0.1775 0.91773 0.64531 -0.19896 0.37416 -2.7525 -0.091586 0.040349 -0.064792 -0.31466 3.3944 0.044941 -0.55038 -0.65334 0.10436 0.016394 0.24388 1.0085 0.31412 -0.33806 -0.16925 0.10228 -0.62143 0.19829 -0.36147 -0.24769 -0.38989 -0.33317 -0.041659 -0.013171\n",
      "as 0.20782 0.12713 -0.30188 -0.23125 0.30175 0.33194 -0.52776 -0.44042 -0.48348 0.03502 0.34782 0.54574 -0.2066 -0.083713 0.2462 0.15931 -0.0031349 0.32443 -0.4527 -0.22178 0.022652 -0.041714 0.31815 0.088633 -0.03801 -1.8212 -0.50917 -0.097544 -0.08953 0.050476 3.718 -0.16503 -0.078733 -0.57101 0.20418 0.13411 0.074281 0.087502 -0.25443 -0.15011 -0.15768 0.39606 -0.23646 -0.095054 0.07859 -0.012305 -0.49879 -0.35301 0.05058 0.019495\n",
      "it 0.61183 -0.22072 -0.10898 -0.052967 0.50804 0.34684 -0.33558 -0.19152 -0.035865 0.1051 0.07935 0.2449 -0.4373 -0.33344 0.57479 0.69052 0.29713 0.090669 -0.54992 -0.46176 0.10113 -0.02024 0.28479 0.043512 0.45735 -2.0466 -0.58084 0.61797 0.6518 -0.58263 4.0786 -0.2542 -0.14649 -0.34321 -0.25437 -0.44677 0.12657 0.28134 0.13331 -0.36974 0.050059 -0.10058 -0.017907 0.11142 -0.71798 0.491 -0.099974 -0.043688 -0.097922 0.16806\n",
      "by 0.35215 -0.35603 0.25708 -0.10611 -0.20718 0.63596 -1.0129 -0.45964 -0.48749 -0.080555 0.43769 0.46046 -0.80943 -0.23336 0.46623 -0.10866 -0.1221 -0.63544 -0.73486 -0.24848 0.4317 0.092264 0.52033 -0.46784 0.016798 -1.5124 -0.19986 -0.43351 -0.59247 0.18088 3.5194 -0.7024 0.23613 -0.68514 -0.37009 -0.080451 0.10635 -0.085495 -0.18451 0.29771 0.18123 0.53627 -0.1001 -0.55165 0.098833 -0.12942 -0.82628 -0.4329 -0.10301 -0.56079\n",
      "at 0.27724 0.88469 -0.26247 0.084104 0.40813 -1.1697 -0.68522 0.1427 -0.57345 -0.58575 -0.50834 -0.86411 -0.52596 -0.56379 0.32862 0.43393 -0.21248 0.49365 -1.8137 -0.035741 1.3227 0.80865 0.012217 -0.087017 -0.16813 -1.5935 0.47034 0.26097 -0.41666 -0.38526 3.4413 0.34383 -0.035895 -0.5678 0.18377 -0.48647 0.42646 0.4408 1.0931 0.063915 -0.064305 -0.29231 0.086502 0.35245 0.17891 0.25941 0.37069 -0.51611 0.023163 0.05779\n",
      "( -0.24978 1.0476 0.21602 0.23278 0.12371 0.2761 0.51184 -1.36 -0.6902 -0.66679 0.49105 0.51671 -0.027218 -0.52056 0.49539 -0.097307 0.12779 0.44388 -1.2612 0.66209 -0.55461 -0.43498 0.81247 0.40855 -0.094327 -0.622 0.36498 -1.0038 -0.77693 -0.22408 3.6533 -0.52004 -0.57384 0.72381 -0.24887 -0.14347 0.69169 -0.51861 1.0806 0.20382 1.1045 0.31045 0.60765 -0.64538 -0.60249 0.60803 0.34393 -0.79411 0.15177 0.45779\n",
      ") -0.28314 1.0028 0.14746 0.22262 0.0070985 0.23108 0.57082 -1.2767 -0.72415 -0.7527 0.52624 0.39498 0.0018922 -0.39396 0.44859 -0.019057 0.068143 0.45082 -1.2849 0.68088 -0.48318 -0.45829 0.85504 0.47712 -0.16152 -0.74784 0.40742 -0.97385 -0.7258 -0.17232 3.8901 -0.46535 -0.61925 0.63584 -0.20339 -0.080612 0.64959 -0.51208 0.91193 0.036208 1.0099 0.18802 0.59359 -0.61313 -0.66839 0.67479 0.40625 -0.6959 0.14553 0.37339\n",
      "from 0.41037 0.11342 0.051524 -0.53833 -0.12913 0.22247 -0.9494 -0.18963 -0.36623 -0.067011 0.19356 -0.33044 0.11615 -0.58585 0.36106 0.12555 -0.3581 -0.023201 -1.2319 0.23383 0.71256 0.14824 0.50874 -0.12313 -0.20353 -1.82 0.22291 0.020291 -0.081743 -0.27481 3.7343 -0.01874 -0.084522 -0.30364 0.27959 0.043328 -0.24621 0.015373 0.49751 0.15108 -0.01619 0.40132 0.23067 -0.10743 -0.36625 -0.051135 0.041474 -0.36064 -0.19616 -0.81066\n",
      "his -0.033537 0.47537 -0.68746 -0.72661 0.84028 0.64304 -0.75975 0.63242 -0.54176 0.11632 -0.20254 0.63321 -1.2677 -0.17674 0.35284 -0.55096 -0.65025 -0.3405 -0.31658 -0.077908 -0.11085 0.97299 -0.016844 -0.73752 0.47852 -2.7069 -0.42417 -0.053489 0.018467 -0.11892 3.3082 0.17864 -0.50702 -0.22894 0.24178 0.5698 0.097113 0.95422 0.0076093 -0.54154 0.09828 0.41533 -1.116 0.0050954 -0.14975 -0.45133 -0.081188 -0.62173 -0.022628 -0.4383\n",
      "'' 0.0028594 0.19457 -0.19449 -0.037583 0.9634 0.099237 -0.27993 -0.71535 -0.28148 0.073535 -0.47299 0.85916 -1.1857 0.12859 1.419 0.23505 0.77673 0.22569 0.20118 -0.62546 -0.53357 0.90877 0.14301 -0.31878 0.612 -2.1162 -1.1655 0.49382 0.87872 -0.77584 3.1332 0.021558 -0.4612 0.0059404 -0.84945 -0.38848 0.086459 -0.39445 0.83242 0.062272 -0.49093 0.68111 0.087143 -0.23992 0.22192 -0.12472 -0.28543 0.043905 -0.22286 1.6923\n",
      "`` 0.12817 0.15858 -0.38843 -0.39108 0.68366 0.00081259 -0.22981 -0.63358 -0.27663 0.40934 -0.65128 0.8461 -0.9904 0.20696 1.2567 0.064774 0.65813 0.39954 0.076104 -0.54083 -0.32438 0.8456 0.17273 -0.13504 0.39626 -2.3358 -1.6576 0.59957 1.0876 -1.0118 3.33 0.075853 -0.65637 -0.015799 -0.85429 -0.47358 0.082404 -0.69719 0.46647 -0.32044 -0.45517 0.30804 0.07502 -0.021783 0.10823 -0.03306 -0.2514 0.088184 -0.22215 1.4971\n",
      "an 0.36143 0.58615 -0.23718 0.079656 0.80192 0.49919 -0.33172 -0.19785 0.13876 0.16804 0.12557 -0.24494 -0.092315 0.35135 -0.024396 -0.31713 0.071206 0.37087 -0.82027 0.21193 -0.052153 0.29928 -0.49494 -0.12546 -0.012394 -2.2174 -0.082666 0.15184 0.050396 0.61229 3.7305 -0.93152 -0.28716 -0.48056 0.060682 0.058104 0.42065 -0.046598 0.083503 -0.23819 0.38828 0.36926 -0.44066 0.075673 -0.050556 -0.42269 -0.21577 0.39362 0.36523 0.36077\n",
      "be 0.91102 -0.22872 0.2077 -0.20237 0.50697 -0.057893 -0.41729 -0.075341 -0.30454 -0.003286 0.44481 0.41818 -0.33409 0.032917 0.98872 0.91984 0.40521 0.01925 -0.1052 -0.79865 -0.36403 -0.087995 0.72182 0.11114 0.2153 -1.9411 -0.26376 0.4455 0.27586 -0.21104 4.0212 -0.061943 -0.32134 -0.81922 0.2108 -0.20414 0.72625 0.47517 -0.39853 -0.39168 -0.34581 0.025928 0.13072 0.73562 -0.15199 -0.18439 -0.67128 0.16692 -0.050063 0.19241\n",
      "has 0.54822 0.038847 0.10127 0.31319 0.095487 0.41814 -0.79493 -0.58296 0.026643 0.12392 0.35194 -0.02163 -0.87018 -0.27178 0.65449 0.42934 0.097544 0.31779 -0.11921 -0.097106 -0.47585 0.24907 0.1223 -0.29079 -0.16866 -2.1072 0.022174 0.45277 -0.64485 0.13181 3.6594 -0.1714 0.23919 -0.42249 -0.088331 -0.32925 -0.12847 0.47055 -0.075953 -0.27747 -0.41905 0.60803 -0.24261 0.014885 -0.23204 0.020879 -0.82175 0.26588 -0.40267 -0.17111\n",
      "are 0.96193 0.012516 0.21733 -0.06539 0.26843 0.33586 -0.45112 -0.60547 -0.46845 -0.18412 0.060949 0.19597 0.22645 0.032802 0.42488 0.49678 0.65346 -0.0274 0.17809 -1.1979 -0.40634 -0.22659 1.1495 0.59342 -0.23759 -0.93254 -0.52502 0.05125 0.032248 -0.72774 4.2466 0.60592 0.33397 -0.85754 0.4895 0.21744 -0.13451 0.0094912 -0.54173 0.18857 -0.64506 0.012695 0.73452 1.0032 0.41874 0.16596 -0.71085 0.14032 -0.38468 -0.38712\n",
      "have 0.94911 -0.34968 0.48125 -0.19306 -0.0088384 0.28182 -0.9613 -0.13581 -0.43083 -0.092933 0.15689 0.059585 -0.49635 -0.17414 0.75661 0.4921 0.21773 -0.22778 -0.13686 -0.90589 -0.48781 0.19919 0.91447 -0.16203 -0.20645 -1.7312 -0.47622 -0.04854 -0.14027 -0.45828 4.0326 0.6052 0.10448 -0.7361 0.2485 -0.033461 -0.13395 0.052782 -0.27268 0.079825 -0.80127 0.30831 0.43567 0.88747 0.29816 -0.02465 -0.95075 0.36233 -0.72512 -0.6089\n",
      "but 0.35934 -0.2657 -0.046477 -0.2496 0.54676 0.25924 -0.64458 0.1736 -0.53056 0.13942 0.062324 0.18459 -0.75495 -0.19569 0.70799 0.44759 0.27031 -0.32885 -0.38891 -0.61606 -0.484 0.41703 0.34794 -0.19706 0.40734 -2.1488 -0.24284 0.33809 0.43993 -0.21616 3.7635 0.19002 -0.12503 -0.38228 0.12944 -0.18272 0.076803 0.51579 0.0072516 -0.29192 -0.27523 0.40593 -0.040394 0.28353 -0.024724 0.10563 -0.32879 0.10673 -0.11503 0.074678\n",
      "were 0.73363 -0.74815 0.45913 -0.56041 0.091855 0.33015 -1.2034 -0.15565 -1.1205 -0.5938 0.23299 -0.46278 -0.34786 -0.47901 0.57621 -0.16053 -0.26457 -0.13732 -0.91878 -0.65339 0.05884 0.61553 1.2607 -0.39821 -0.26056 -1.0127 -0.38517 -0.096929 -0.11701 -0.48536 3.6902 0.30744 0.50713 -0.6537 0.80491 0.23672 0.61769 0.030195 -0.57645 0.60467 -0.63949 -0.11373 0.84984 0.41409 0.083774 -0.28737 -1.4735 -0.20095 -0.17246 -1.0984\n",
      "not 0.55025 -0.24942 -0.0009386 -0.264 0.5932 0.2795 -0.25666 0.093076 -0.36288 0.090776 0.28409 0.71337 -0.4751 -0.24413 0.88424 0.89109 0.43009 -0.2733 0.11276 -0.81665 -0.41272 0.17754 0.61942 0.10466 0.33327 -2.3125 -0.52371 -0.021898 0.53801 -0.50615 3.8683 0.16642 -0.71981 -0.74728 0.11631 -0.37585 0.5552 0.12675 -0.22642 -0.10175 -0.35455 0.12348 0.16532 0.7042 -0.080231 -0.068406 -0.67626 0.33763 0.050139 0.33465\n",
      "this 0.53074 0.40117 -0.40785 0.15444 0.47782 0.20754 -0.26951 -0.34023 -0.10879 0.10563 -0.10289 0.10849 -0.49681 -0.25128 0.84025 0.38949 0.32284 -0.22797 -0.44342 -0.31649 -0.12406 -0.2817 0.19467 0.055513 0.56705 -1.7419 -0.91145 0.27036 0.41927 0.020279 4.0405 -0.24943 -0.20416 -0.62762 -0.054783 -0.26883 0.18444 0.18204 -0.23536 -0.16155 -0.27655 0.035506 -0.38211 -0.00075134 -0.24822 0.28164 0.12819 0.28762 0.1444 0.23611\n",
      "who -0.19461 -0.051277 0.26445 -0.57399 1.0236 0.58923 -1.3399 0.31032 -0.89433 -0.13192 0.21305 0.29171 -0.66079 0.084125 0.76578 -0.42393 0.32445 0.13603 -0.29987 -0.046415 -0.74811 1.2134 0.24988 0.22846 0.23546 -2.6054 0.12491 -0.94028 -0.58308 -0.32325 2.8419 0.33474 -0.33902 -0.23434 0.37735 0.093804 -0.25969 0.68889 0.37689 -0.2186 -0.24244 1.0029 0.18607 0.27486 0.48089 -0.43533 -1.1012 -0.67103 -0.21652 -0.025891\n",
      "they 0.70835 -0.57361 0.15375 -0.63335 0.46879 -0.066566 -0.86826 0.35967 -0.64786 -0.22525 0.09752 0.27732 -0.35176 -0.25955 0.62368 0.60824 0.34905 -0.27195 -0.27981 -1.0183 -0.1487 0.41932 1.0342 0.17783 0.13569 -1.9999 -0.56163 0.004018 0.60839 -1.0031 3.9546 0.68698 -0.53593 -0.7427 0.18078 0.034527 0.016026 0.12467 -0.084633 -0.10375 -0.47862 -0.22314 0.25487 0.69985 0.32714 -0.15726 -0.6202 -0.23113 -0.31217 -0.3049\n",
      "had 0.60348 -0.52096 0.40851 -0.37217 0.36978 0.61082 -1.3228 0.24375 -0.5942 -0.35708 0.39942 0.031911 -1.0643 -0.52327 0.71453 0.063384 -0.46383 -0.34641 -0.72445 -0.13714 -0.19179 0.72225 0.6295 -0.8086 -0.037694 -2.0355 0.10566 -0.038591 -0.23201 -0.29627 3.3215 0.032443 0.085368 -0.40771 0.45341 -0.099674 0.44704 0.5422 0.18185 0.17504 -0.33833 0.31697 -0.025268 0.095795 -0.25071 -0.47564 -1.0407 -0.15138 -0.22057 -0.59633\n",
      "i 0.11891 0.15255 -0.082073 -0.74144 0.75917 -0.48328 -0.31009 0.51476 -0.98708 0.00061757 -0.15043 0.8377 -1.0797 -0.5146 1.3188 0.62007 0.13779 0.47108 -0.072874 -0.72675 -0.74116 0.75263 0.8818 0.29561 1.3548 -2.5701 -1.3523 0.4588 1.0068 -1.1856 3.4737 0.77898 -0.72929 0.25102 -0.26156 -0.34684 0.55841 0.75098 0.4983 -0.26823 -0.0027443 -0.018298 -0.28096 0.55318 0.037706 0.18555 -0.15025 -0.57512 -0.26671 0.92121\n",
      "which 0.90558 0.054033 -0.024091 0.08111 0.08645 0.65504 -0.34224 -0.76129 0.10258 0.059494 0.30353 -0.10311 -0.28574 -0.35059 0.23319 0.27913 -0.0021905 0.16015 -0.65622 -0.13339 0.38494 -0.20867 0.26137 -0.090254 -0.34935 -1.5398 -0.46352 0.16734 -0.19253 -0.1979 4.008 -0.24514 -0.15461 -0.2889 -0.049511 -0.29696 0.2161 -0.15298 -0.12235 0.071447 -0.11104 -0.15518 -0.026936 -0.067826 -0.56607 0.20991 -0.40505 -0.12906 -0.18325 -0.58796\n",
      "will 0.81544 0.30171 0.5472 0.46581 0.28531 -0.56112 -0.43913 -0.0090877 0.10002 -0.17218 0.28133 0.37672 -0.40756 0.15836 0.89113 1.2997 0.51508 -0.1948 0.051856 -0.9338 0.069955 -0.24876 -0.016723 -0.2031 -0.033558 -1.8132 0.11199 -0.31961 -0.13746 -0.45499 3.8856 1.214 -1.0046 -0.056274 0.0038776 -0.40669 0.29452 0.30171 0.038848 -0.56088 -0.46582 0.17155 0.33729 -0.15247 0.023771 0.51415 -0.21759 0.31965 -0.34741 0.41672\n",
      "their 0.41519 0.13167 -0.0569 -0.56765 0.49924 0.21288 -0.81949 0.32257 -0.065374 -0.055513 0.11837 0.36933 -0.46424 -0.072383 0.068214 0.0014523 -0.07322 -0.65668 0.11368 -0.91816 0.029319 0.38103 0.34032 -0.21496 -0.26681 -1.6509 -0.71668 -0.41272 0.48465 -0.62432 4.1939 1.4292 -0.45902 -0.51709 0.2626 0.51086 -0.23999 -0.06962 -0.4561 -0.48333 -0.39544 -0.53831 -0.070727 0.54496 0.2351 -0.18746 -0.2242 -0.11806 -0.34499 -0.86949\n",
      ": -0.17587 1.3508 -0.18159 0.45197 0.37554 -0.20926 0.014956 -0.87286 -0.54443 -0.25731 -0.521 0.62242 -0.52387 -0.061782 1.1805 -0.041984 0.10582 -0.20913 -0.54508 0.027728 -0.31329 0.13439 0.55192 0.75419 0.30996 -1.3301 -0.9862 -0.33747 0.17633 -0.37547 3.4474 0.14171 -0.65033 0.10118 0.00014796 -0.074707 0.19146 -0.47977 0.39628 -0.13403 0.43043 0.45704 0.59387 -0.40308 0.067302 1.2784 0.49927 0.15617 0.5665 0.61385\n",
      "or 0.26358 0.18747 0.044394 -0.19119 0.45455 0.66445 0.25855 -0.64886 -0.67653 0.045254 0.071081 0.3645 0.74863 -0.17489 0.28723 0.43277 -0.39184 -0.048568 -0.21373 -0.72992 0.13902 -0.23308 0.70256 0.2176 -0.20647 -1.415 -0.32587 -0.075019 0.88536 -0.56679 4.0296 0.019803 -0.57259 -0.060878 0.14667 0.16532 0.21188 -0.38358 0.42748 -0.096921 0.19285 0.021779 0.58562 0.97633 0.20384 -0.2162 -0.021486 -0.42936 0.52879 -0.12598\n",
      "its 0.76719 0.1239 -0.11119 0.13355 0.18356 0.057912 -0.3341 -0.60423 0.47637 0.25451 0.19491 -0.061142 -0.45815 -0.17374 -0.32716 0.33472 -0.3218 0.090518 -0.24682 -0.35467 0.55269 -0.33177 -0.58048 -0.55391 -0.64466 -1.8028 -0.65173 0.4374 0.051813 0.22641 4.2766 0.19443 -0.13428 -0.10278 -0.062464 -0.39073 -0.29381 -0.013531 -0.58142 -0.69717 -0.068871 -0.50049 -0.013803 -0.11011 -0.64282 0.4396 -0.22455 0.4893 -0.26152 -0.46886\n",
      "one 0.31474 0.41662 0.1348 0.15854 0.88812 0.43317 -0.55916 0.030476 -0.14623 -0.14273 -0.17949 -0.17343 -0.49264 0.26775 0.48799 -0.29537 0.18485 0.14937 -0.75009 -0.35651 -0.23699 0.1849 0.17237 0.23611 0.14077 -1.9031 -0.65353 -0.022539 0.10383 -0.43705 3.781 -0.044077 -0.046643 0.027274 0.51883 0.13353 0.23231 0.25599 0.060888 -0.065618 -0.15556 0.30818 -0.093586 0.33296 -0.14613 0.016332 -0.24251 -0.20526 0.07009 -0.11568\n",
      "after 0.38315 -0.3561 -0.1283 -0.19527 0.047629 0.21468 -0.98765 0.82962 -0.42782 -0.22879 0.10712 -0.3087 -1.2069 -0.17713 0.88841 0.0056658 -0.77305 -0.66913 -1.3384 0.34676 0.5044 0.5125 0.26826 -0.65313 -0.081516 -2.1658 0.57974 0.036345 0.0090949 0.25772 3.4402 0.20732 -0.52028 0.026453 0.17895 -0.017802 0.36605 0.34539 0.41357 -0.2497 -0.49227 0.17745 -0.43764 -0.3484 -0.057061 -0.039578 -0.13517 -0.4258 0.13681 -0.77731\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMwxUcgDHzGu"
   },
   "source": [
    "\n",
    "Try and initialize the CNN embedding layer with the `50D` pre-trained GloVe embeddings. Pay particular attention to keeping the correct indices from the `word2idx` for the lookup table! Once you fill the below `wvecs` matrix, copy the previous training loop and initialize its embedding layer with the pre-trained ones as follows:\n",
    "\n",
    "```python\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
    "```\n",
    "\n",
    "**Note:** The learning rate is initially set to 0.5.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: What should the embedding for the padding token `<pad>` be?**\n",
    " \n",
    "**Q: What is the impact of using those pre-trained embeddings on the model performance?**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OCsV8mtBg4-Y"
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# Yet another hyperparameter: since the pre-trained embeddings are coming\n",
    "# from a different network, their magnitudes could differ from the parameters\n",
    "# of this network. So scaling may be necessary.\n",
    "SCALE_EMBS = 0.65\n",
    "\n",
    "# Creates the empty numpy array that you should fill below\n",
    "wvecs = np.zeros((len(word2idx), EMBEDDING_DIM), dtype='float32')\n",
    "\n",
    "#####################################################################\n",
    "# Q: Read line by line, find the corresponding word and\n",
    "# insert its embedding to the correct position in the `wvecs` matrix.\n",
    "# Once done, apply the SCALE_EMBS factor to scale the vectors\n",
    "#####################################################################\n",
    "with open(f'glove.6B.{EMBEDDING_DIM}d.txt', 'r') as f:\n",
    "  ...\n",
    "        \n",
    "print(wvecs)\n",
    "\n",
    "#####################\n",
    "# Re-create the model\n",
    "#####################\n",
    "fix_seed()\n",
    "\n",
    "EPOCHS = 10\n",
    "LRATE = 0.5\n",
    "\n",
    "# the hyperparameters specific to CNN\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# we define the number of filters\n",
    "N_OUT_CHANNELS = 100\n",
    "\n",
    "# we define the window size\n",
    "WINDOW_SIZE = 1\n",
    "\n",
    "# we apply the dropout with the probability 0.1\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Construct the model\n",
    "model = CNN(len(word2idx), EMBEDDING_DIM, N_OUT_CHANNELS, WINDOW_SIZE, OUTPUT_DIM, DROPOUT)\n",
    "\n",
    "#################################################################\n",
    "### Q: Initialize the embeddings with the loaded pre-trained ones\n",
    "#################################################################\n",
    "...\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=LRATE)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "feature_train = train_sent_tensor\n",
    "target_train = train_label_tensor\n",
    "\n",
    "feature_valid = valid_sent_tensor\n",
    "target_valid = valid_label_tensor\n",
    "\n",
    "feature_test = test_sent_tensor\n",
    "target_test = test_label_tensor\n",
    "\n",
    "################\n",
    "# Start training\n",
    "################\n",
    "print(f'Will train for {EPOCHS} epochs')\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "  model.train()\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  predictions = model(feature_train).squeeze(1)\n",
    "  loss = loss_fn(predictions, target_train)\n",
    "  train_loss = loss.item()\n",
    "  train_acc = accuracy(predictions, target_train)\n",
    "\n",
    "  loss.backward()\n",
    "\n",
    "  optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    predictions_valid = model(feature_valid).squeeze(1)\n",
    "    valid_loss = loss_fn(predictions_valid, target_valid).item()\n",
    "    valid_acc = accuracy(predictions_valid, target_valid)\n",
    "  \n",
    "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:6.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:6.2f}% |')\n",
    "\n",
    "\n",
    "## Finally, test on the test set\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(feature_test).squeeze(1)\n",
    "    loss = loss_fn(predictions, target_test)\n",
    "    acc = accuracy(predictions, target_test)\n",
    "    print(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "    f_measure(predictions, test_labels)"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Will train for 10 epochs\n",
      "| Epoch: 01 | Train Loss: 0.709 | Train Acc:  50.00% | Val. Loss: 1.291 | Val. Acc:  50.00% |\n",
      "| Epoch: 02 | Train Loss: 1.455 | Train Acc:  50.00% | Val. Loss: 4.705 | Val. Acc:  50.00% |\n",
      "| Epoch: 03 | Train Loss: 4.738 | Train Acc:  50.00% | Val. Loss: 0.641 | Val. Acc:  50.00% |\n",
      "| Epoch: 04 | Train Loss: 0.257 | Train Acc: 100.00% | Val. Loss: 1.259 | Val. Acc:  50.00% |\n",
      "| Epoch: 05 | Train Loss: 0.168 | Train Acc: 100.00% | Val. Loss: 0.817 | Val. Acc:  66.67% |\n",
      "| Epoch: 06 | Train Loss: 0.069 | Train Acc: 100.00% | Val. Loss: 1.074 | Val. Acc:  50.00% |\n",
      "| Epoch: 07 | Train Loss: 0.054 | Train Acc: 100.00% | Val. Loss: 1.081 | Val. Acc:  66.67% |\n",
      "| Epoch: 08 | Train Loss: 0.041 | Train Acc: 100.00% | Val. Loss: 1.047 | Val. Acc:  66.67% |\n",
      "| Epoch: 09 | Train Loss: 0.029 | Train Acc: 100.00% | Val. Loss: 1.166 | Val. Acc:  66.67% |\n",
      "| Epoch: 10 | Train Loss: 0.023 | Train Acc: 100.00% | Val. Loss: 1.180 | Val. Acc:  50.00% |\n",
      "Test Loss: 1.870 | Test Acc: 50.00%\n",
      "     Recall: 1.00, Precision: 0.50, F-measure: 0.67\n"
     ]
    }
   ]
  }
 ]
}
