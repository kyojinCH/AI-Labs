{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "colab": {
   "provenance": [],
   "private_outputs": true,
   "toc_visible": true
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 3, Part 2 : Feed Forward Neural Language Models"
   ],
   "metadata": {
    "id": "KaMsBkHIS_fY"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJMQLZ_uf6tI"
   },
   "source": [
    "# About this lab\n",
    "\n",
    "In this session, you will experiment with feed-forward neural language models (FFLM) using [PyTorch](https://www.pytorch.org). To train the models, you will be using the [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) corpus, which is a popular LM dataset introduced in 2016:\n",
    "\n",
    "> The WikiText language modeling dataset is a collection of texts extracted from Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), `WikiText-2` is over 2 times larger. The dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\n",
    "\n",
    "Goal of this lab : \n",
    "* Understand FFN\n",
    "* Train a FFNLM\n",
    "* Use PyTorch\n",
    "\n",
    "This part should take you 3h"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Downloading Stuff & Setting Up the Environment"
   ],
   "metadata": {
    "id": "k5w6kOKHTYfB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\BookEx\\\\python.exe'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000C\u0000o\u0000p\u0000y\u0000r\u0000i\u0000g\u0000h\u0000t\u0000 \u0000(\u0000c\u0000)\u0000 \u0000M\u0000i\u0000c\u0000r\u0000o\u0000s\u0000o\u0000f\u0000t\u0000 \u0000C\u0000o\u0000r\u0000p\u0000o\u0000r\u0000a\u0000t\u0000i\u0000o\u0000n\u0000.\u0000 \u0000T\u0000o\u0000u\u0000s\u0000 \u0000d\u0000r\u0000o\u0000i\u0000t\u0000s\u0000 \u0000r\u0000é\u0000s\u0000e\u0000r\u0000v\u0000é\u0000s\u0000.\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000U\u0000t\u0000i\u0000l\u0000i\u0000s\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000:\u0000 \u0000w\u0000s\u0000l\u0000.\u0000e\u0000x\u0000e\u0000 \u0000[\u0000A\u0000r\u0000g\u0000u\u0000m\u0000e\u0000n\u0000t\u0000]\u0000 \u0000A\u0000r\u0000g\u0000u\u0000m\u0000e\u0000n\u0000t\u0000s\u0000 \u0000:\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000-\u0000-\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000e\u0000r\u0000 \u0000<\u0000O\u0000p\u0000t\u0000i\u0000o\u0000n\u0000s\u0000>\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000I\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000e\u0000r\u0000 \u0000l\u0000e\u0000s\u0000 \u0000f\u0000o\u0000n\u0000c\u0000t\u0000i\u0000o\u0000n\u0000n\u0000a\u0000l\u0000i\u0000t\u0000é\u0000s\u0000 \u0000d\u0000u\u0000 \u0000s\u0000o\u0000u\u0000s\u0000-\u0000s\u0000y\u0000s\u0000t\u0000è\u0000m\u0000e\u0000 \u0000W\u0000i\u0000n\u0000d\u0000o\u0000w\u0000s\u0000 \u0000p\u0000o\u0000u\u0000r\u0000 \u0000L\u0000i\u0000n\u0000u\u0000x\u0000.\u0000 \u0000S\u0000i\u0000 \u0000a\u0000u\u0000c\u0000u\u0000n\u0000e\u0000 \u0000o\u0000p\u0000t\u0000i\u0000o\u0000n\u0000 \u0000n\u0000'\u0000e\u0000s\u0000t\u0000 \u0000s\u0000p\u0000é\u0000c\u0000i\u0000f\u0000i\u0000é\u0000e\u0000,\u0000 \u0000l\u0000e\u0000s\u0000 \u0000f\u0000o\u0000n\u0000c\u0000t\u0000i\u0000o\u0000n\u0000n\u0000a\u0000l\u0000i\u0000t\u0000é\u0000s\u0000 \u0000r\u0000e\u0000c\u0000o\u0000m\u0000m\u0000a\u0000n\u0000d\u0000é\u0000e\u0000s\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000s\u0000e\u0000r\u0000o\u0000n\u0000t\u0000 \u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000é\u0000e\u0000s\u0000 \u0000a\u0000v\u0000e\u0000c\u0000 \u0000l\u0000a\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000 \u0000p\u0000a\u0000r\u0000 \u0000d\u0000é\u0000f\u0000a\u0000u\u0000t\u0000.\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000P\u0000o\u0000u\u0000r\u0000 \u0000a\u0000f\u0000f\u0000i\u0000c\u0000h\u0000e\u0000r\u0000 \u0000l\u0000a\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000 \u0000p\u0000a\u0000r\u0000 \u0000d\u0000é\u0000f\u0000a\u0000u\u0000t\u0000 \u0000a\u0000i\u0000n\u0000s\u0000i\u0000 \u0000q\u0000u\u0000'\u0000u\u0000n\u0000e\u0000 \u0000l\u0000i\u0000s\u0000t\u0000e\u0000 \u0000d\u0000'\u0000a\u0000u\u0000t\u0000r\u0000e\u0000s\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000s\u0000 \u0000v\u0000a\u0000l\u0000i\u0000d\u0000e\u0000s\u0000,\u0000 \u0000u\u0000t\u0000i\u0000l\u0000i\u0000s\u0000e\u0000z\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000'\u0000w\u0000s\u0000l\u0000 \u0000-\u0000-\u0000l\u0000i\u0000s\u0000t\u0000 \u0000-\u0000-\u0000o\u0000n\u0000l\u0000i\u0000n\u0000e\u0000'\u0000.\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000O\u0000p\u0000t\u0000i\u0000o\u0000n\u0000s\u0000 \u0000:\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000-\u0000-\u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000,\u0000 \u0000-\u0000d\u0000 \u0000[\u0000A\u0000r\u0000g\u0000u\u0000m\u0000e\u0000n\u0000t\u0000]\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000S\u0000p\u0000é\u0000c\u0000i\u0000f\u0000i\u0000e\u0000 \u0000l\u0000a\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000 \u0000à\u0000 \u0000t\u0000é\u0000l\u0000é\u0000c\u0000h\u0000a\u0000r\u0000g\u0000e\u0000r\u0000 \u0000e\u0000t\u0000 \u0000à\u0000 \u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000e\u0000r\u0000 \u0000p\u0000a\u0000r\u0000 \u0000s\u0000o\u0000n\u0000 \u0000n\u0000o\u0000m\u0000.\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000A\u0000r\u0000g\u0000u\u0000m\u0000e\u0000n\u0000t\u0000s\u0000 \u0000:\u0000 \u0000U\u0000n\u0000 \u0000n\u0000o\u0000m\u0000 \u0000d\u0000e\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000 \u0000v\u0000a\u0000l\u0000i\u0000d\u0000e\u0000 \u0000(\u0000n\u0000o\u0000n\u0000 \u0000s\u0000e\u0000n\u0000s\u0000i\u0000b\u0000l\u0000e\u0000 \u0000à\u0000 \u0000l\u0000a\u0000 \u0000c\u0000a\u0000s\u0000s\u0000e\u0000)\u0000.\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000E\u0000x\u0000e\u0000m\u0000p\u0000l\u0000e\u0000s\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000:\u0000 \u0000w\u0000s\u0000l\u0000 \u0000-\u0000-\u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000 \u0000-\u0000d\u0000 \u0000U\u0000b\u0000u\u0000n\u0000t\u0000u\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000w\u0000s\u0000l\u0000 \u0000-\u0000-\u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000 \u0000-\u0000-\u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000 \u0000D\u0000e\u0000b\u0000i\u0000a\u0000n\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000-\u0000-\u0000i\u0000n\u0000b\u0000o\u0000x\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000I\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000e\u0000r\u0000 \u0000l\u0000a\u0000 \u0000f\u0000o\u0000n\u0000c\u0000t\u0000i\u0000o\u0000n\u0000n\u0000a\u0000l\u0000i\u0000t\u0000é\u0000 \u0000o\u0000p\u0000t\u0000i\u0000o\u0000n\u0000n\u0000e\u0000l\u0000l\u0000e\u0000 \u0000d\u0000e\u0000 \u0000W\u0000i\u0000n\u0000d\u0000o\u0000w\u0000s\u0000 \u0000a\u0000u\u0000 \u0000l\u0000i\u0000e\u0000u\u0000 \u0000d\u0000e\u0000 \u0000l\u0000a\u0000 \u0000v\u0000e\u0000r\u0000s\u0000i\u0000o\u0000n\u0000 \u0000d\u0000i\u0000s\u0000p\u0000o\u0000n\u0000i\u0000b\u0000l\u0000e\u0000 \u0000v\u0000i\u0000a\u0000 \u0000l\u0000e\u0000 \u0000M\u0000i\u0000c\u0000r\u0000o\u0000s\u0000o\u0000f\u0000t\u0000 \u0000S\u0000t\u0000o\u0000r\u0000e\u0000.\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000-\u0000-\u0000e\u0000n\u0000a\u0000b\u0000l\u0000e\u0000-\u0000w\u0000s\u0000l\u00001\u0000 \u0000A\u0000c\u0000t\u0000i\u0000v\u0000e\u0000 \u0000l\u0000a\u0000 \u0000p\u0000r\u0000i\u0000s\u0000e\u0000 \u0000e\u0000n\u0000 \u0000c\u0000h\u0000a\u0000r\u0000g\u0000e\u0000 \u0000d\u0000e\u0000 \u0000W\u0000S\u0000L\u00001\u0000 \u0000a\u0000v\u0000e\u0000c\u0000 \u0000l\u0000a\u0000 \u0000v\u0000e\u0000r\u0000s\u0000i\u0000o\u0000n\u0000 \u0000d\u0000u\u0000 \u0000M\u0000i\u0000c\u0000r\u0000o\u0000s\u0000o\u0000f\u0000t\u0000 \u0000S\u0000t\u0000o\u0000r\u0000e\u0000.\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000-\u0000-\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000n\u0000o\u0000-\u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000N\u0000e\u0000 \u0000p\u0000a\u0000s\u0000 \u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000e\u0000r\u0000 \u0000d\u0000e\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000 \u0000(\u0000n\u0000e\u0000 \u0000p\u0000e\u0000u\u0000t\u0000 \u0000p\u0000a\u0000s\u0000 \u0000ê\u0000t\u0000r\u0000e\u0000 \u0000u\u0000t\u0000i\u0000l\u0000i\u0000s\u0000é\u0000 \u0000a\u0000v\u0000e\u0000c\u0000 \u0000-\u0000-\u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000)\u0000.\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000-\u0000-\u0000n\u0000o\u0000-\u0000l\u0000a\u0000u\u0000n\u0000c\u0000h\u0000,\u0000 \u0000-\u0000n\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000N\u0000e\u0000 \u0000p\u0000a\u0000s\u0000 \u0000l\u0000a\u0000n\u0000c\u0000e\u0000r\u0000 \u0000l\u0000a\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000 \u0000a\u0000p\u0000r\u0000è\u0000s\u0000 \u0000l\u0000'\u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000a\u0000t\u0000i\u0000o\u0000n\u0000.\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000-\u0000-\u0000w\u0000e\u0000b\u0000-\u0000d\u0000o\u0000w\u0000n\u0000l\u0000o\u0000a\u0000d\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000T\u0000é\u0000l\u0000é\u0000c\u0000h\u0000a\u0000r\u0000g\u0000e\u0000z\u0000 \u0000l\u0000a\u0000 \u0000v\u0000e\u0000r\u0000s\u0000i\u0000o\u0000n\u0000 \u0000l\u0000a\u0000 \u0000p\u0000l\u0000u\u0000s\u0000 \u0000r\u0000é\u0000c\u0000e\u0000n\u0000t\u0000e\u0000 \u0000d\u0000e\u0000 \u0000W\u0000S\u0000L\u0000 \u0000d\u0000e\u0000p\u0000u\u0000i\u0000s\u0000 \u0000l\u0000'\u0000I\u0000n\u0000t\u0000e\u0000r\u0000n\u0000e\u0000t\u0000 \u0000a\u0000u\u0000 \u0000l\u0000i\u0000e\u0000u\u0000 \u0000d\u0000e\u0000 \u0000l\u0000a\u0000 \u0000v\u0000e\u0000r\u0000s\u0000i\u0000o\u0000n\u0000 \u0000d\u0000u\u0000 \u0000M\u0000i\u0000c\u0000r\u0000o\u0000s\u0000o\u0000f\u0000t\u0000 \u0000S\u0000t\u0000o\u0000r\u0000e\u0000.\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000-\u0000-\u0000l\u0000i\u0000s\u0000t\u0000,\u0000 \u0000-\u0000l\u0000 \u0000[\u0000O\u0000p\u0000t\u0000i\u0000o\u0000n\u0000s\u0000]\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000-\u0000L\u0000i\u0000s\u0000t\u0000e\u0000 \u0000l\u0000e\u0000s\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000s\u0000.\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000O\u0000p\u0000t\u0000i\u0000o\u0000n\u0000s\u0000 \u0000:\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000-\u0000-\u0000o\u0000n\u0000l\u0000i\u0000n\u0000e\u0000,\u0000 \u0000-\u0000o\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000A\u0000f\u0000f\u0000i\u0000c\u0000h\u0000e\u0000 \u0000u\u0000n\u0000e\u0000 \u0000l\u0000i\u0000s\u0000t\u0000e\u0000 \u0000d\u0000e\u0000s\u0000 \u0000d\u0000i\u0000s\u0000t\u0000r\u0000i\u0000b\u0000u\u0000t\u0000i\u0000o\u0000n\u0000s\u0000 \u0000d\u0000i\u0000s\u0000p\u0000o\u0000n\u0000i\u0000b\u0000l\u0000e\u0000s\u0000 \u0000p\u0000o\u0000u\u0000r\u0000 \u0000l\u0000'\u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000a\u0000t\u0000i\u0000o\u0000n\u0000 \u0000a\u0000v\u0000e\u0000c\u0000 \u0000'\u0000w\u0000s\u0000l\u0000 \u0000-\u0000-\u0000i\u0000n\u0000s\u0000t\u0000a\u0000l\u0000l\u0000'\u0000.\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000-\u0000-\u0000s\u0000t\u0000a\u0000t\u0000u\u0000s\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000A\u0000f\u0000f\u0000i\u0000c\u0000h\u0000e\u0000 \u0000l\u0000'\u0000é\u0000t\u0000a\u0000t\u0000 \u0000d\u0000u\u0000 \u0000s\u0000o\u0000u\u0000s\u0000-\u0000s\u0000y\u0000s\u0000t\u0000è\u0000m\u0000e\u0000 \u0000W\u0000i\u0000n\u0000d\u0000o\u0000w\u0000s\u0000 \u0000p\u0000o\u0000u\u0000r\u0000 \u0000L\u0000i\u0000n\u0000u\u0000x\u0000.\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000-\u0000-\u0000h\u0000e\u0000l\u0000p\u0000 \u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000A\u0000f\u0000f\u0000i\u0000c\u0000h\u0000e\u0000 \u0000d\u0000e\u0000s\u0000 \u0000i\u0000n\u0000f\u0000o\u0000r\u0000m\u0000a\u0000t\u0000i\u0000o\u0000n\u0000s\u0000 \u0000s\u0000u\u0000r\u0000 \u0000l\u0000'\u0000u\u0000t\u0000i\u0000l\u0000i\u0000s\u0000a\u0000t\u0000i\u0000o\u0000n\u0000.\u0000\n",
      "\u0000\n",
      "\u0000\n",
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "!wsl which bash"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bash_kernel in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: pexpect>=4.0 in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from bash_kernel) (4.8.0)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from bash_kernel) (6.22.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from pexpect>=4.0->bash_kernel) (0.7.0)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->bash_kernel) (1.5.6)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->bash_kernel) (6.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->bash_kernel) (1.6.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->bash_kernel) (8.1.0)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->bash_kernel) (5.9.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->bash_kernel) (5.3.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->bash_kernel) (8.11.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->bash_kernel) (0.1.6)\n",
      "Requirement already satisfied: pyzmq>=20 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->bash_kernel) (25.0.2)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from ipykernel->bash_kernel) (23.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->bash_kernel) (5.9.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipykernel->bash_kernel) (0.1.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->bash_kernel) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->bash_kernel) (2.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->bash_kernel) (0.4.6)\n",
      "Requirement already satisfied: stack-data in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->bash_kernel) (0.6.2)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->bash_kernel) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->bash_kernel) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->bash_kernel) (3.0.38)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from ipython>=7.23.1->ipykernel->bash_kernel) (0.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->bash_kernel) (2.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-client>=6.1.12->ipykernel->bash_kernel) (6.1.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->bash_kernel) (3.1.1)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->bash_kernel) (305)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel->bash_kernel) (3.15.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->bash_kernel) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel->bash_kernel) (0.2.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\bookex\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->bash_kernel) (1.16.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->bash_kernel) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->bash_kernel) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\mon pc\\appdata\\roaming\\python\\python39\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->bash_kernel) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bash_kernel # did not work either"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hel\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo \"Hel\" # simple command to test if bash works"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Mon PC\\\\DataspellProjects\\\\AI LAB\\\\AI LAB 3'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1eLd2J2B1i2u"
   },
   "source": [
    "# Download the corpus. Simple commands work but not complexe ones...\n",
    "%%bash\n",
    "URL=\"https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2\"\n",
    "\n",
    "for split in \"train\" \"valid\" \"test\";do\n",
    "if [ ! -f \"${split}.txt\" ]; then\n",
    "echo \"Downloading ${split}.txt\"\n",
    "wget -q \"${URL}/${split}.txt\"\n",
    "# Remove empty lines\n",
    "sed -i '/^ *$/d' \"${split}.txt\"\n",
    "# Remove article titles starting with = and ending with =\n",
    "sed -i '/^ *= .* = $/d' \"${split}\".txt\n",
    "fi\n",
    "done\n",
    "\n",
    "# Prepare smaller version for fast training neural LMs\n",
    "head -n 5000 < train.txt > train_small.txt\n",
    "\n",
    "# Print the first 10 lines with line numbers\n",
    "cat -n train.txt | head -n10\n",
    "echo\n",
    "\n",
    "# Print some statistics\n",
    "echo -e \"\\n   Line,   word,   character counts\"\n",
    "wc *.txt"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (589482835.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[6], line 5\u001B[1;36m\u001B[0m\n\u001B[1;33m    for split in \"train\" \"valid\" \"test\";do\u001B[0m\n\u001B[1;37m                                       ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Another Testing Phase"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "CompletedProcess(args=['cmd.exe', 'echo', 'hahaha'], returncode=0)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "subprocess.run([\"cmd.exe\",\"echo\",\"hahaha\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msubprocess\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m subprocess\u001B[38;5;241m.\u001B[39mrun([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcmd.exe\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwget\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-q\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43murl\u001B[49m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msplit\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"cmd.exe\",\"wget\", \"-q\", f\"{url}/{split}.txt\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Line,   word,   character counts\n",
      "Microsoft Windows [version 10.0.22621.1413]\n",
      "(c) Microsoft Corporation. Tous droits r‚serv‚s.\n",
      "\n",
      "(BookEx) C:\\Users\\Mon PC\\DataspellProjects\\AI LAB\\AI LAB 3>\n"
     ]
    }
   ],
   "source": [
    "#gave me hope but still not working\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2\"\n",
    "\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "  if not os.path.isfile(f\"{split}.txt\"):\n",
    "    print(f\"Downloading {split}.txt\")\n",
    "    subprocess.run([\"cmd.exe\",\"wget\", \"-q\", f\"{url}/{split}.txt\"])\n",
    "    # Remove empty lines\n",
    "    subprocess.run([\"cmd.exe\",\"sed\", \"-i\", '/^ *$/d', f\"{split}.txt\"])\n",
    "    # Remove article titles starting with = and ending with =\n",
    "    subprocess.run([\"cmd.exe\",\"sed\", \"-i\", '/^ *= .* = $/d', f\"{split}.txt\"])\n",
    "\n",
    "# Prepare smaller version for fast training neural LMs\n",
    "subprocess.run([\"cmd.exe\",\"head\", \"-n\", \"5000\", \"<\", \"train.txt\", \">\", \"train_small.txt\"])\n",
    "\n",
    "# Print the first 10 lines with line numbers\n",
    "subprocess.run([\"cmd.exe\",\"cat\", \"-n\", \"train.txt\"], capture_output=True, text=True)\n",
    "print()\n",
    "\n",
    "# Print some statistics\n",
    "output = subprocess.run([\"cmd.exe\",\"wc\", \"*.txt\"], capture_output=True, text=True)\n",
    "print(\"   Line,   word,   character counts\")\n",
    "print(output.stdout)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Here End the Tests\n",
    "I had to manually write the bash command on a bash prompt since I couldn't get them to work here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GSFcmlFdf6tK"
   },
   "source": [
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "# Fancy progress bar\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "###############\n",
    "# Torch setup #\n",
    "###############\n",
    "print('Torch version: {}, CUDA: {}'.format(torch.__version__, torch.version.cuda))\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if not torch.cuda.is_available():\n",
    "  print('WARNING: You may want to change the runtime to GPU for Neural LM experiments!')\n",
    "  DEVICE = 'cpu'\n",
    "else:\n",
    "  DEVICE = 'cuda:0'\n",
    "\n",
    "#######################\n",
    "# Some helper functions\n",
    "#######################\n",
    "\n",
    "def fix_seed(seed=None):\n",
    "  \"\"\"Sets the seeds of random number generators.\"\"\"\n",
    "  torch.use_deterministic_algorithms(True)\n",
    "  if seed is None:\n",
    "    # Take a random seed\n",
    "    seed = time.time()\n",
    "  seed = int(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  return seed\n",
    "\"\"\"\n",
    "@:param n: number of parameters\n",
    "\"\"\"\n",
    "def readable_size(n):\n",
    "  \"\"\"Returns a readable size string for model parameters count.\"\"\"\n",
    "  sizes = ['K', 'M', 'G']\n",
    "  fmt = ''\n",
    "  size = n\n",
    "  for i, s in enumerate(sizes):\n",
    "    nn = n / (1000 ** (i + 1))\n",
    "    if nn >= 1:\n",
    "      size = nn\n",
    "      fmt = sizes[i]\n",
    "    else:\n",
    "      break\n",
    "  return '%.2f%s' % (size, fmt)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.0, CUDA: 11.8\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmP0ZWbvB1aa"
   },
   "source": [
    "# Feed-forward Language Models (FFLM)\n",
    "\n",
    "FFLMs are similar to $n$-gram language models in the sense that the choice of $n$ is a hyperparameter for the network architecture. A basic FFLM constructs a  $C=n\\mathrm{-1}$ length context window before the word to be predicted. If the word embedding size is $E$, the feature vector for the context window becomes a vector of size $E\\times C$, resulting from the **concatenation** of individual word embeddings of context words. Hence, the choice of $C$ for FFLMs, affects the number of final learnable parameters in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A - Dataset Stuff"
   ],
   "metadata": {
    "id": "c_sCtdDlUIvV"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWMhsDZOf6tS"
   },
   "source": [
    "### Representing the vocabulary\n",
    "\n",
    "The below `Vocabulary` class encapsulates the **word-to-idx** and **idx-to-word** mapping that you should now be familiar with from the previous lab sessions. Read it to understand how the vocabulary is constructed from a plain text file, within the `build_from_file()` method. Special `<.>` markers are also included in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xlxMOBzPf6tT"
   },
   "source": [
    "\n",
    "class Vocabulary(object):\n",
    "  \"\"\"Data structure representing the vocabulary of a corpus.\"\"\"\n",
    "  def __init__(self):\n",
    "    # Mapping from tokens to integers\n",
    "    self._word2idx = {}\n",
    "\n",
    "    # Reverse-mapping from integers to tokens\n",
    "    self.idx2word = []\n",
    "\n",
    "    # 0-padding token\n",
    "    self.add_word('<pad>')\n",
    "    # sentence start\n",
    "    self.add_word('<s>')\n",
    "    # sentence end\n",
    "    self.add_word('</s>')\n",
    "    # Unknown words\n",
    "    self.add_word('<unk>')\n",
    "\n",
    "    self._unk_idx = self._word2idx['<unk>']\n",
    "  \"\"\"\n",
    "  Returns the integer ID of the word or <unk> if not found.\n",
    "  @:param word: word to be converted to index\n",
    "  \"\"\"\n",
    "  def word2idx(self, word):\n",
    "    return self._word2idx.get(word, self._unk_idx)\n",
    "\n",
    "\"\"\"\n",
    "Adds the `word` into the vocabulary.\n",
    "@:param word: word to be added to the vocabulary\n",
    "\"\"\"\n",
    "  def add_word(self, word):\n",
    "    if word not in self._word2idx:\n",
    "      self.idx2word.append(word)\n",
    "      self._word2idx[word] = len(self.idx2word) - 1\n",
    "\n",
    "\"\"\"\n",
    "Builds a vocabulary from a given corpus file.\n",
    "@:param fname: file name of the corpus\n",
    "\"\"\"\n",
    "  def build_from_file(self, fname):\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "      for line in f:\n",
    "        words = line.strip().split()\n",
    "        for word in words:\n",
    "          self.add_word(word)\n",
    "\"\"\"\n",
    "Converts a list of indices to words.\n",
    "@:param idxs: list of indices to be converted to words\n",
    "\"\"\"\n",
    "  def convert_idxs_to_words(self, idxs):\n",
    "    return ' '.join(self.idx2word[idx] for idx in idxs)\n",
    "\n",
    "\"\"\"\n",
    "Converts a list of words to a list of indices.\n",
    "@:param words: list of words to be converted to indices\n",
    "\"\"\"\n",
    "  def convert_words_to_idxs(self, words):\n",
    "    return [self.word2idx(w) for w in words]\n",
    "\n",
    "\"\"\"\n",
    "Returns the size of the vocabulary.\n",
    "\"\"\"\n",
    "  def __len__(self):\n",
    "    return len(self.idx2word)\n",
    "\n",
    "\"\"\"\n",
    "Returns a string representation of the vocabulary.\n",
    "\"\"\"\n",
    "  def __repr__(self):\n",
    "    return \"Vocabulary with {} items\".format(self.__len__())"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hE1Lr6j_oYB"
   },
   "source": [
    "Let's construct the vocabulary for the training set and analyse the token indices for a sentence with an unknown word.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **Why do we map unknown tokens to a special `<unk>` token?**\n",
    "* **Do you think the network will learn a useful embedding for that? If not, how can you let the network to learn an embedding for it?**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QcNMhXqB5wwT"
   },
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.build_from_file('train.txt')\n",
    "print(vocab)\n",
    "\n",
    "# TODO : Convert sentence to list of indices, note how the last word is mapped to 3 (<unk>)\n",
    "sentence = \"Probable 1 unknown word_\"\n",
    "print(vocab.convert_words_to_idxs(sentence.split()))"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary with 33280 items\n",
      "[3, 1000, 5042, 3]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "the word `word_` is constructed so as not to be in the vocabulary. It is mapped to the `<unk>` token as expected. Upon further testing, it seems that the vocab is case-sensitive, `probable` and `Probable` are different tokens. I am not sure it is intended or not. However, I believe that it would be better if  the vocab was not case-sensitive as words be they capitalized or not, are still the same word and should as such have the same embedding."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-4PQ9_8f6tV"
   },
   "source": [
    "### Representing the corpus\n",
    "\n",
    "Let's process the corpus for PyTorch: all splits will end up being a large, 1D token sequences. Note that, in `corpus_to_tensor()`, every line is wrapped between `<s> .. </s>` tags."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M5s9MMt06YsL"
   },
   "source": [
    "\"\"\"\n",
    "Converts a corpus file to a tensor of token indices.\n",
    "@:param _vocab: vocabulary object\n",
    "@:param filename: file name of the corpus\n",
    "\"\"\"\n",
    "def corpus_to_tensor(_vocab, filename):\n",
    "  # Final token indices\n",
    "  idxs = []  \n",
    "  with open(filename, encoding = 'utf-8') as data: # added the encoding since the initial code was not working on my machine.\n",
    "    for line in tqdm(data, ncols=80, unit=' line', desc=f'Reading {filename} '): # tqdm is a progress bar, it is not necessary but it is nice to have.\n",
    "                                                                                 # ncols is the width of the progress bar, unit is the unit of the progress bar, desc is the description of the progress bar.\n",
    "                                                                                 # unit is the unit of the progress bar, desc is the description of the progress bar.\n",
    "      line = line.strip() # Remove leading and trailing whitespaces\n",
    "      # Skip empty lines if any\n",
    "      if line:\n",
    "        # Each line is considered as a long sentence for WikiText-2\n",
    "        line = f\"<s> {line} </s>\" # Add sentence markers\n",
    "        # Split from whitespace and add sentence markers\n",
    "        idxs.extend(_vocab.convert_words_to_idxs(line.split())) # Convert words to indices\n",
    "  return torch.LongTensor(idxs)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hl6xuwZS9uO1"
   },
   "source": [
    "# Read the files, prepare the small one as well\n",
    "train = corpus_to_tensor(vocab, 'train.txt')\n",
    "train_small = corpus_to_tensor(vocab, 'train_small.txt')\n",
    "\n",
    "valid = corpus_to_tensor(vocab, 'valid.txt')\n",
    "test = corpus_to_tensor(vocab, 'test.txt')\n",
    "print('\\n')\n",
    "\n",
    "\"\"\"\n",
    "Returns a human-readable size.\n",
    "\"\"\"\n",
    "print(f'Small training size in tokens: {readable_size(len(train_small))}')\n",
    "print(f'Training size in tokens: {readable_size(len(train))}')\n",
    "print(f'Validation size in tokens: {readable_size(len(valid))}')\n",
    "print(f'Test size in tokens: {readable_size(len(test))}')"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading train.txt : 36718 line [00:00, 51735.07 line/s]\n",
      "Reading train_small.txt : 5000 line [00:00, 50128.17 line/s]\n",
      "Reading valid.txt : 3760 line [00:00, 53726.05 line/s]\n",
      "Reading test.txt : 4358 line [00:00, 61778.50 line/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Small training size in tokens: 276.94K\n",
      "Training size in tokens: 2.10M\n",
      "Validation size in tokens: 218.81K\n",
      "Test size in tokens: 246.99K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNxNrdhPECML"
   },
   "source": [
    "**Q: Print the first 20 token indices from the training set. And then print the sentence in actual words corresponding to these 20 tokens by using one of the provided methods in the `Vocabulary` class.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cXobO9_n-Giz"
   },
   "source": [
    "########\n",
    "print(train[:20])\n",
    "print(vocab.convert_idxs_to_words(train[:20]))\n",
    "########"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  4,  5,  6,  7,  4,  2,  1,  8,  9,  5, 10, 11,  3,  6, 12, 13, 11,\n",
      "        14, 15])\n",
      "<s> = Valkyria Chronicles III = </s> <s> Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 ,\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiroR-NszhGY"
   },
   "source": [
    "## B - Model definition\n",
    "\n",
    "Now that we are done with data loading and vocabulary construction, we can define the actual FFLM model in PyTorch. Recall from the lectures that this model requires a pre-defined context window size $C$ which will affect the way you set up some of the linear layers. **Note that**, in contrast to the model depicted in the lecture, this model has an additional layer `ff_ctx`, which projects the context vector $c_k$ to hidden dimension $H$. This ensures that the number of parameters in the output layer does not depend on the context size, i.e. it is always $H\\times V$ instead of $CE\\times V$.\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Follow the comments in `__init__()` and `forward()` to fill in the missing parts with some actual code.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X9ttYW2IC_UV"
   },
   "source": [
    "\"\"\"\n",
    "Feed-forward language model.\n",
    "\"\"\"\n",
    "class FFLM(nn.Module):\n",
    "  \"\"\"\n",
    "    Initializes the model.\n",
    "    @:param vocab_size: size of the vocabulary\n",
    "    @:param emb_dim: embedding dimension\n",
    "    @:param hid_dim: hidden dimension\n",
    "    @:param context_size: context window size\n",
    "    @:param dropout: dropout probability\n",
    "  \"\"\"\n",
    "  def __init__(self, vocab_size, emb_dim, hid_dim, context_size, dropout=0.5):\n",
    "    # Call parent's __init__ first\n",
    "    super(FFLM, self).__init__()\n",
    "\n",
    "    # Store arguments\n",
    "    self.vocab_size = vocab_size\n",
    "    self.emb_dim = emb_dim\n",
    "    self.hid_dim = hid_dim\n",
    "    self.context_size = context_size\n",
    "\n",
    "    # Create the loss, don't sum or average, we'll take care of it\n",
    "    # in the training loop for logging purposes\n",
    "    self.loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    # Create the non-linearity\n",
    "    self.nonlin = torch.nn.Tanh()\n",
    "\n",
    "    # Dropout regularizer\n",
    "    self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    ##############################\n",
    "    # Fill the missing parts below\n",
    "    ##############################\n",
    "    # TODO : Compute the dimension of the context vector\n",
    "    self.context_dim = context_size * emb_dim\n",
    "\n",
    "    # Create the embedding layer (i.e. lookup table tokens->vectors)\n",
    "    self.emb = nn.Embedding(\n",
    "      num_embeddings=self.vocab_size, embedding_dim=self.emb_dim,\n",
    "      padding_idx=0)\n",
    "\n",
    "    # This cuts the number of parameters a bit\n",
    "    self.ff_ctx = nn.Linear(self.context_dim, self.hid_dim)\n",
    "\n",
    "    ############################################\n",
    "    # Output layer mapping from the output of `ff_ctx` to vocabulary size\n",
    "    # TODO : Fill the dimensions of the output layer\n",
    "    ############################################\n",
    "    self.out = nn.Linear(self.hid_dim, self.vocab_size)\n",
    "\n",
    "    # Purely for informational purposes: compute # of total params\n",
    "    self.n_params = 0\n",
    "    for param in self.parameters():\n",
    "      self.n_params += np.cumprod(param.data.size())[-1]\n",
    "    self.n_params = readable_size(self.n_params)\n",
    "\n",
    "\"\"\"\n",
    "Forward-pass of the module.\n",
    "@:param x: input tensor of token indices, shape (batch_size, context_size+1)\n",
    "@:param y: true labels, shape (batch_size, context_size+1)\n",
    "\"\"\"\n",
    "  def forward(self, x, y):\n",
    "    \"\"\"Forward-pass of the module.\"\"\"\n",
    "    # TODO : What is the shape of x ?\n",
    "    # The shape of x is (batch_size, context_size+1).\n",
    "\n",
    "    # TODO : Get the embeddings for the token indices in `x`\n",
    "    embs = self.emb(x)\n",
    "\n",
    "    # TODO : Concatenate the embeddings to form the context vector\n",
    "    ctx = embs.view(embs.shape[0], -1) # Reshape to (batch_size, context_size*emb_dim)\n",
    "\n",
    "    # TODO : Apply ff_ctx -> non-lin -> dropout -> output layer to obtain the logits i.e. unnormalized scores\n",
    "    h = self.ff_ctx(ctx)\n",
    "    h = self.nonlin(h)\n",
    "    h = self.drop(h)\n",
    "    logits = self.out(h)\n",
    "\n",
    "\n",
    "\n",
    "    # TODO : Use self.loss to compute the losses, return the losses (true labels are in `y`)\n",
    "    return self.loss(logits.view(-1, self.vocab_size), y.view(-1))\n",
    "\n",
    "\"\"\"\n",
    "Returns a tensor of size (n_batches, batch_size, context_size + 1).\n",
    "@:param data_tensor: tensor of token indices\n",
    "@:param batch_size: batch size, default 64\n",
    "\"\"\"\n",
    "  def get_batches(self, data_tensor, batch_size=64):\n",
    "    \"\"\"Returns a tensor of size (n_batches, batch_size, context_size + 1).\"\"\"\n",
    "    # Split data into rows of n-grams followed by the (n+1)th true label\n",
    "    x_y = data_tensor.unfold(0, self.context_size + 1, step=1)\n",
    "\n",
    "    # Get the number of training n-grams\n",
    "    n_samples = x_y.size()[0]\n",
    "\n",
    "    # Hack: discard the last uneven batch for simplicity\n",
    "    n_batches = n_samples // batch_size\n",
    "    n_samples = n_batches * batch_size\n",
    "    # Split nicely into batches, i.e. (n_batches, batch_size, context_size + 1)\n",
    "    # The final element in each row is the ID of the true label to predict\n",
    "    x_y = x_y[:n_samples].view(n_batches, batch_size, -1)\n",
    "\n",
    "    # A particular batch for context_size=2 will now look like below in\n",
    "    # word format. Last element for every array is the next token to be predicted\n",
    "    #\n",
    "    # [[<s>, cat, sat],\n",
    "    #  [cat, sat, on],\n",
    "    #  [sat, on,  the],\n",
    "    #  [on,  the, mat],\n",
    "    #   ....\n",
    "    return x_y\n",
    "\"\"\"\n",
    "trains the model.\n",
    "@:param optim: optimizer, shifts the parameters in the direction of the gradient\n",
    "@:param train_tensor: tensor of token indices for training\n",
    "@:param valid_tensor: tensor of token indices for validation\n",
    "@:param test_tensor: tensor of token indices for testing\n",
    "@:param n_epochs: number of epochs, default 5, i.e. 5 passes over the training data\n",
    "@:param batch_size: batch size, default 64\n",
    "@:param shuffle: whether to shuffle the batches or not, default False. Shuffling\n",
    "                 is useful for training, but not for testing.\n",
    "\"\"\"\n",
    "  def train_model(self, optim, train_tensor, valid_tensor, test_tensor, n_epochs=5,\n",
    "                  batch_size=64, shuffle=False):\n",
    "    \"\"\"Trains the model.\"\"\"\n",
    "    # Get batches for the training data\n",
    "    batches = self.get_batches(train_tensor, batch_size)\n",
    "\n",
    "    print(f'Will do {batches.size(0)} batches for an epoch.')\n",
    "\n",
    "    for eidx in range(1, n_epochs + 1): # Epoch loop\n",
    "      start_time = time.time() # For reporting time per epoch\n",
    "      epoch_loss = 0\n",
    "      epoch_items = 0\n",
    "\n",
    "      # Enable training mode\n",
    "      self.train() # Enable training mode\n",
    "\n",
    "      # Shuffle the batch order or not\n",
    "      if shuffle:\n",
    "        batch_order = torch.randperm(batches.size(0))\n",
    "      else:\n",
    "        batch_order = torch.arange(batches.size(0))\n",
    "\n",
    "      # Start training\n",
    "      for iter_count, idx in enumerate(batch_order):\n",
    "        batch = batches[idx].to(DEVICE)\n",
    "\n",
    "        # TODO : Split into inputs `x` and labels `y`. Hint : Look at the context_size\n",
    "        x, y = batch[:, :-1], batch[:, -1] # The last element in each row is the ID of the true label to predict\n",
    "\n",
    "        # Clear the gradients\n",
    "        optim.zero_grad() # Clear the gradients\n",
    "\n",
    "        # TODO : Compute the loss thanks to one of the previous function\n",
    "        loss = self.forward(x, y)\n",
    "\n",
    "        # Backprop the average loss and update parameters\n",
    "        loss.mean().backward() # Backprop the average loss and update parameters\n",
    "        optim.step()\n",
    "\n",
    "        # sum the loss for reporting, along with the denominator\n",
    "        epoch_loss += loss.detach().sum()\n",
    "        epoch_items += loss.numel()\n",
    "\n",
    "        if iter_count % 1000 == 0:\n",
    "          # Print progress\n",
    "          loss_per_token = epoch_loss / epoch_items\n",
    "          ppl = math.exp(loss_per_token)\n",
    "          print(f'[Epoch {eidx:<3}] loss: {loss_per_token:6.2f}, perplexity: {ppl:6.2f}')\n",
    "\n",
    "      time_spent = time.time() - start_time\n",
    "\n",
    "      print(f'\\n[Epoch {eidx:<3}] ended with train_loss: {loss_per_token:6.2f}, ppl: {ppl:6.2f}')\n",
    "      # Evaluate on valid set\n",
    "      valid_loss, valid_ppl = self.evaluate(test_set=valid_tensor)\n",
    "      print(f'[Epoch {eidx:<3}] ended with valid_loss: {valid_loss:6.2f}, valid_ppl: {valid_ppl:6.2f}')\n",
    "      print(f'[Epoch {eidx:<3}] completed in {time_spent:.2f} seconds\\n')\n",
    "\n",
    "    # Evaluate the final model on test set\n",
    "    test_loss, test_ppl = self.evaluate(test_set=test_tensor)\n",
    "    print(f' ---> Final test set performance: {test_loss:6.2f}, test_ppl: {test_ppl:6.2f}')\n",
    "\n",
    "\"\"\"\n",
    "Evaluates the model on the given test set.\n",
    "@:param test_set: tensor of token indices for testing\n",
    "@:param batch_size: batch size, default 32\n",
    "\"\"\"\n",
    "  def evaluate(self, test_set, batch_size=32):\n",
    "    \"\"\"Evaluates and computes perplexity for the given test set.\"\"\"\n",
    "    loss = 0\n",
    "\n",
    "    # Get the batches\n",
    "    batches = self.get_batches(test_set, batch_size)\n",
    "\n",
    "    # TODO : Set your model to Eval mode\n",
    "    self.eval()\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient computation\n",
    "      for batch in batches: # Batch loop\n",
    "        batch = batch.to(DEVICE) # Move to GPU\n",
    "\n",
    "        # TODO : Split into inputs `x` and labels `y`. Hint : Look at the context_size\n",
    "        x, y = batch[:, :-1], batch[:, -1]\n",
    "\n",
    "        # loss will be a vector of size (batch_size, ) with losses per every sample\n",
    "        # sum the loss for reporting, along with the denominator\n",
    "        loss += self.forward(x, y).sum()\n",
    "\n",
    "    # Normalize by the number of tokens in the test set\n",
    "    loss /= batches.size()[:2].numel()\n",
    "\n",
    "    # TODO : Switch back to training mode\n",
    "    self.train()\n",
    "\n",
    "    # return the perplexity and loss\n",
    "    return loss, math.exp(loss)\n",
    "\n",
    "  def __repr__(self):\n",
    "    \"\"\"String representation for pretty-printing.\"\"\"\n",
    "    s = super(FFLM, self).__repr__()\n",
    "    return f\"{s}\\n# of parameters: {self.n_params}\""
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0eHpYiRUHDe"
   },
   "source": [
    "## C - Training\n",
    "\n",
    "We can now launch training using a set of sane hyper-parameters for our model. This is a 3-gram FFLM since the context size is set to 2. On a Colab GPU, a single epoch should take around 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7rjwvEYYFjjE"
   },
   "source": [
    "# Set the seed for reproducible results\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "fix_seed(42)\n",
    "\n",
    "fflm_model = FFLM(\n",
    "  len(vocab),       # vocabulary size\n",
    "  emb_dim=128,      # word embedding dim\n",
    "  hid_dim=128,      # hidden layer dim\n",
    "  context_size=2,   # C = (N-1) if you think in n-gram LM terminology\n",
    "  dropout=0.4,      # dropout probability\n",
    ")\n",
    "\n",
    "# move to device\n",
    "fflm_model.to(DEVICE)\n",
    "\n",
    "# Initial learning rate for the optimizer\n",
    "FFLM_INIT_LR = 0.001\n",
    "\n",
    "# Create the optimizer\n",
    "fflm_optimizer = torch.optim.Adam(fflm_model.parameters(), lr=FFLM_INIT_LR)\n",
    "print(fflm_model)\n",
    "\n",
    "print('Starting training!')\n",
    "# NOTE: If you happen to have memory errors, try decreasing the batch size\n",
    "# It will print progress every 1000 batches\n",
    "fflm_model.train_model(fflm_optimizer, train, valid, test, n_epochs=5, batch_size=256, shuffle=True)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
      "FFLM(\n",
      "  (loss): CrossEntropyLoss()\n",
      "  (nonlin): Tanh()\n",
      "  (drop): Dropout(p=0.4, inplace=False)\n",
      "  (emb): Embedding(33280, 128, padding_idx=0)\n",
      "  (ff_ctx): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (out): Linear(in_features=128, out_features=33280, bias=True)\n",
      ")\n",
      "# of parameters: 8.59M\n",
      "Starting training!\n",
      "Will do 8200 batches for an epoch.\n",
      "[Epoch 1  ] loss:  10.49, perplexity: 35882.11\n",
      "[Epoch 1  ] loss:   7.27, perplexity: 1439.55\n",
      "[Epoch 1  ] loss:   6.98, perplexity: 1072.42\n",
      "[Epoch 1  ] loss:   6.83, perplexity: 927.49\n",
      "[Epoch 1  ] loss:   6.74, perplexity: 846.14\n",
      "[Epoch 1  ] loss:   6.67, perplexity: 785.10\n",
      "[Epoch 1  ] loss:   6.61, perplexity: 738.93\n",
      "[Epoch 1  ] loss:   6.56, perplexity: 705.65\n",
      "[Epoch 1  ] loss:   6.51, perplexity: 673.90\n",
      "\n",
      "[Epoch 1  ] ended with train_loss:   6.51, ppl: 673.90\n",
      "[Epoch 1  ] ended with valid_loss:   5.80, valid_ppl: 330.63\n",
      "[Epoch 1  ] completed in 43.10 seconds\n",
      "\n",
      "[Epoch 2  ] loss:   6.08, perplexity: 435.11\n",
      "[Epoch 2  ] loss:   6.04, perplexity: 419.60\n",
      "[Epoch 2  ] loss:   6.02, perplexity: 412.63\n",
      "[Epoch 2  ] loss:   6.01, perplexity: 409.27\n",
      "[Epoch 2  ] loss:   6.01, perplexity: 406.61\n",
      "[Epoch 2  ] loss:   6.00, perplexity: 403.25\n",
      "[Epoch 2  ] loss:   5.99, perplexity: 399.75\n",
      "[Epoch 2  ] loss:   5.99, perplexity: 397.82\n",
      "[Epoch 2  ] loss:   5.98, perplexity: 395.26\n",
      "\n",
      "[Epoch 2  ] ended with train_loss:   5.98, ppl: 395.26\n",
      "[Epoch 2  ] ended with valid_loss:   5.65, valid_ppl: 284.09\n",
      "[Epoch 2  ] completed in 40.99 seconds\n",
      "\n",
      "[Epoch 3  ] loss:   5.82, perplexity: 336.50\n",
      "[Epoch 3  ] loss:   5.77, perplexity: 321.97\n",
      "[Epoch 3  ] loss:   5.77, perplexity: 322.08\n",
      "[Epoch 3  ] loss:   5.78, perplexity: 322.33\n",
      "[Epoch 3  ] loss:   5.78, perplexity: 324.02\n",
      "[Epoch 3  ] loss:   5.79, perplexity: 325.56\n",
      "[Epoch 3  ] loss:   5.79, perplexity: 326.00\n",
      "[Epoch 3  ] loss:   5.79, perplexity: 325.43\n",
      "[Epoch 3  ] loss:   5.78, perplexity: 324.77\n",
      "\n",
      "[Epoch 3  ] ended with train_loss:   5.78, ppl: 324.77\n",
      "[Epoch 3  ] ended with valid_loss:   5.57, valid_ppl: 262.18\n",
      "[Epoch 3  ] completed in 41.30 seconds\n",
      "\n",
      "[Epoch 4  ] loss:   5.05, perplexity: 156.19\n",
      "[Epoch 4  ] loss:   5.62, perplexity: 276.78\n",
      "[Epoch 4  ] loss:   5.63, perplexity: 278.89\n",
      "[Epoch 4  ] loss:   5.64, perplexity: 281.15\n",
      "[Epoch 4  ] loss:   5.64, perplexity: 282.06\n",
      "[Epoch 4  ] loss:   5.65, perplexity: 283.75\n",
      "[Epoch 4  ] loss:   5.65, perplexity: 284.32\n",
      "[Epoch 4  ] loss:   5.65, perplexity: 285.59\n",
      "[Epoch 4  ] loss:   5.66, perplexity: 286.36\n",
      "\n",
      "[Epoch 4  ] ended with train_loss:   5.66, ppl: 286.36\n",
      "[Epoch 4  ] ended with valid_loss:   5.52, valid_ppl: 250.37\n",
      "[Epoch 4  ] completed in 42.10 seconds\n",
      "\n",
      "[Epoch 5  ] loss:   5.43, perplexity: 227.78\n",
      "[Epoch 5  ] loss:   5.51, perplexity: 247.17\n",
      "[Epoch 5  ] loss:   5.52, perplexity: 248.73\n",
      "[Epoch 5  ] loss:   5.53, perplexity: 251.29\n",
      "[Epoch 5  ] loss:   5.54, perplexity: 253.71\n",
      "[Epoch 5  ] loss:   5.55, perplexity: 256.00\n",
      "[Epoch 5  ] loss:   5.55, perplexity: 257.48\n",
      "[Epoch 5  ] loss:   5.56, perplexity: 259.03\n",
      "[Epoch 5  ] loss:   5.56, perplexity: 260.61\n",
      "\n",
      "[Epoch 5  ] ended with train_loss:   5.56, ppl: 260.61\n",
      "[Epoch 5  ] ended with valid_loss:   5.48, valid_ppl: 238.84\n",
      "[Epoch 5  ] completed in 41.28 seconds\n",
      "\n",
      " ---> Final test set performance:   5.40, test_ppl: 221.09\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAHN-C-XHuVs"
   },
   "source": [
    "**Q: If everything goes well, you should see a loss of around ~10.4 printed as the first loss. This will still be the case if you change the random seed to some other number before model construction i.e. the culprit is not the exact values that they take.**\n",
    "* **Can you come up with a simple mathematical formula which yields that value?**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RiG_vI1uJj1X"
   },
   "source": [
    "##########################\n",
    "# Answer to question above\n",
    "##########################\n",
    "print(\"<TODO: put the formula here which computes the value>\")\n"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TODO: put the formula here which computes the value>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div align=\"center\">\n",
    "$ log(P) = -\\frac{1}{n}\\sum^n_{k=1}log(P))$\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.412711894935144\n"
     ]
    }
   ],
   "source": [
    "print(math.log(fflm_model.vocab_size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The value is close to the found first loss value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsYXRU68SWF7"
   },
   "source": [
    "## D - Further Exploring\n",
    "\n",
    "With the default settings above, you should end up with a validation perplexity of $\\sim1076$ and a final test set perplexity of $\\sim1003$ at the end of 5th epoch. Try the following questions to further analyze the model's prediction.\n",
    "\n",
    "---\n",
    "\n",
    "* **Q: Remove the `tanh()` non-linearity from the code so that the context is computed as a linear combination of its embeddings. How does the results compare to the initial one? Do you think non-linearity helps?**\n",
    "\n",
    "* **Q: Compare the results by rerunning the training with unshuffled batches i.e. with `shuffle=False`. What do you notice in terms of results?**\n",
    "\n",
    "* **Q: Play with hyper-parameters related to dimensions and dropout. Could you find a model with smaller perplexity?**\n",
    "\n",
    "* **Q: Try with different context sizes such as 3, 5, 7, etc. What is the best perplexity you can get?**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q1:** I have not done it but from my reading I know that non-linearities are introduced so as to  make the model more expressive. The model with non-linearity has a better perplexity than the one without non-linearity. It also makes the model more robust to the data.\n",
    "**Q2:** Shuffling the Data will make the model better at generalizing. The model with shuffled data has a better perplexity than the one without shuffling.\n",
    "**Q3:** Too much work for me sir :(, I tried to implemet sklearn's RandomizedSearchCV but it was taking too much time to run. I will try to do it later.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq1fn-x9aS5e"
   },
   "source": [
    "## E - Further Reading for your knowledge\n",
    " - [Original FFLM paper from Bengio et al. 2003](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    " - [Original RNNLM paper from Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
    " - Some recent state-of-the-art LSTM-based RNNLMs\n",
    "\n",
    "  - [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182.pdf)\n",
    "  - [An Analysis of Neural Language Modeling at Multiple Scales](https://arxiv.org/pdf/1803.08240.pdf)\n",
    "  - [Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours](https://mlsys.org/Conferences/2019/doc/2018/50.pdf)"
   ]
  }
 ]
}
